<!DOCTYPE html>
<html lang="en">
<head>
<meta charset="UTF-8">
<meta name="viewport" content="width=device-width, initial-scale=1.0">
<title>Week 2 ¬∑ Lesson 3: Embeddings</title>
<link href="https://fonts.googleapis.com/css2?family=Instrument+Serif:ital@0;1&family=DM+Mono:wght@400;500&family=Manrope:wght@400;500;600;700&display=swap" rel="stylesheet">
<style>
:root{
  --bg:#0e0e1c;--surface:#161628;--surface2:#1c1c38;--border:#2a2a50;
  --text:#c0c0e0;--muted:#7070a0;--heading:#eeeef8;
  --violet:#8860f0;--vi-dim:rgba(136,96,240,.12);--vi-glow:rgba(136,96,240,.3);
  --coral:#f06868;--co-dim:rgba(240,104,104,.12);
  --teal:#40d0b8;--te-dim:rgba(64,208,184,.12);
  --gold:#e8c040;--go-dim:rgba(232,192,64,.12);
  --mono:'DM Mono',monospace;--serif:'Instrument Serif',serif;--sans:'Manrope',sans-serif;
  --r:10px;
}
*{margin:0;padding:0;box-sizing:border-box}
body{font-family:var(--sans);background:var(--bg);color:var(--text);line-height:1.76;font-size:15.5px}
.container{max-width:760px;margin:0 auto;padding:48px 24px 100px;position:relative;z-index:1}
.pbar{position:fixed;top:0;left:0;right:0;height:3px;background:var(--surface);z-index:999}.pfill{height:100%;width:0%;background:var(--violet);box-shadow:0 0 8px var(--vi-glow);transition:width .3s}
.hero{text-align:center;padding:32px 0 48px}.hero-tag{font-family:var(--mono);font-size:.76rem;color:var(--violet);letter-spacing:.14em;text-transform:uppercase}.hero h1{font-family:var(--serif);font-size:2.5rem;font-weight:400;color:var(--heading);margin:8px 0}.hero h1 em{color:var(--violet);font-style:italic}.hero p{color:var(--muted);font-size:1.05rem;max-width:560px;margin:0 auto}
.section{margin-bottom:56px;opacity:0;transform:translateY(20px);transition:all .5s}.section.vis{opacity:1;transform:none}.stag{font-family:var(--mono);font-size:.72rem;color:var(--violet);letter-spacing:.1em;text-transform:uppercase;margin-bottom:4px}.section h2{font-family:var(--serif);font-size:1.55rem;color:var(--heading);margin-bottom:14px}.section h3{font-family:var(--serif);font-size:1.15rem;color:var(--heading);margin:20px 0 10px}.section p{margin-bottom:12px}.section p strong{color:var(--heading)}
pre{background:var(--surface);border:1px solid var(--border);border-left:3px solid var(--violet);border-radius:var(--r);padding:16px 18px;margin:14px 0;overflow-x:auto;font-family:var(--mono);font-size:.84rem;line-height:1.6;color:var(--text)}
code.il{font-family:var(--mono);font-size:.86em;color:var(--violet);background:var(--vi-dim);padding:2px 6px;border-radius:4px}
.kw{color:#e0a0c8}.fn{color:#80c8f0}.num{color:#e8c060}.str{color:#a0e880}.cm{color:var(--muted);font-style:italic}
.callout{background:var(--vi-dim);border:1px solid rgba(136,96,240,.2);border-radius:var(--r);padding:16px 18px;margin:18px 0;font-size:.93rem}.callout.gold{background:var(--go-dim);border-color:rgba(232,192,64,.2)}.callout.warn{background:var(--co-dim);border-color:rgba(240,104,104,.2)}
.analogy{border-left:3px solid var(--gold);padding:10px 16px;margin:16px 0;background:var(--go-dim);border-radius:0 var(--r) var(--r) 0;font-size:.93rem}.analogy strong{color:var(--heading)}
hr.div{border:none;border-top:1px solid var(--border);margin:52px 0}
.cw{background:var(--surface);border:1px solid var(--border);border-radius:var(--r);padding:20px;margin:18px 0}.cw-title{font-family:var(--mono);font-size:.76rem;color:var(--violet);text-transform:uppercase;letter-spacing:.1em;margin-bottom:10px}
.ctrls{display:flex;flex-wrap:wrap;gap:8px;margin:10px 0}
.cbtn{font-family:var(--mono);font-size:.8rem;padding:7px 14px;border:1px solid var(--border);border-radius:6px;background:var(--surface2);color:var(--text);cursor:pointer;transition:all .2s}.cbtn:hover{border-color:var(--violet);color:var(--violet)}.cbtn.on{background:var(--vi-dim);border-color:var(--violet);color:var(--violet)}
.out{background:#0a0a16;border:1px solid var(--border);border-radius:6px;padding:14px 16px;font-family:var(--mono);font-size:.84rem;color:var(--teal);line-height:1.6;margin:10px 0;white-space:pre-wrap;min-height:36px}
.egrid{display:inline-grid;gap:2px;margin:10px 0}.ecell{width:44px;height:40px;display:flex;align-items:center;justify-content:center;font-family:var(--mono);font-size:.78rem;border-radius:3px;transition:all .3s}.elabel{font-family:var(--mono);font-size:.75rem;color:var(--muted);text-align:center;padding:2px}
.qcard{background:var(--surface);border:1px solid var(--border);border-radius:var(--r);padding:22px;margin:18px 0}.qq{font-family:var(--serif);font-size:1.05rem;color:var(--heading);margin-bottom:12px}.qopts{display:flex;flex-direction:column;gap:7px}.qo{display:flex;align-items:center;gap:10px;padding:11px 14px;border:1px solid var(--border);border-radius:6px;cursor:pointer;font-size:.9rem;transition:all .2s}.qo:hover{border-color:var(--violet)}.qo.c{border-color:var(--teal);background:var(--te-dim);color:var(--teal)}.qo.w{border-color:var(--coral);background:var(--co-dim);color:var(--coral)}.qo.d{pointer-events:none;opacity:.85}
.ql{width:24px;height:24px;border-radius:50%;display:flex;align-items:center;justify-content:center;font-family:var(--mono);font-size:.75rem;font-weight:600;background:var(--surface2);color:var(--muted);flex-shrink:0}.qo.c .ql{background:var(--teal);color:#000}.qo.w .ql{background:var(--coral);color:#fff}
.qfb{margin-top:10px;padding:10px 12px;border-radius:6px;font-size:.88rem;display:none;line-height:1.6}.qfb.show{display:block}.qfb.p{background:var(--te-dim);color:var(--teal);border-left:3px solid var(--teal)}.qfb.f{background:var(--co-dim);color:var(--coral);border-left:3px solid var(--coral)}
.sgrid{display:grid;grid-template-columns:1fr 1fr;gap:10px;margin:14px 0}.sitem{background:var(--surface);border:1px solid var(--border);border-radius:8px;padding:12px 14px}.sitem code{font-family:var(--mono);font-size:.84rem;color:var(--violet)}.sitem .d{font-size:.86rem;color:var(--muted);margin-top:3px}
.step{display:flex;gap:12px;margin:10px 0;align-items:flex-start}.step-n{font-family:var(--mono);font-size:.75rem;color:#000;background:var(--violet);width:26px;height:26px;border-radius:50%;display:flex;align-items:center;justify-content:center;flex-shrink:0;margin-top:2px;font-weight:600}.step-c{flex:1}
canvas{display:block;width:100%;border-radius:6px}
@media(max-width:600px){.hero h1{font-size:1.8rem}.sgrid{grid-template-columns:1fr}.ecell{width:38px;height:34px;font-size:.7rem}}
</style>
</head>
<body>
<div class="pbar"><div class="pfill" id="pf"></div></div>
<div class="container">

<div class="hero">
  <div class="hero-tag">Week 2 ¬∑ Lesson 3</div>
  <h1><em>Embeddings</em></h1>
  <p>Token IDs are arbitrary numbers with no inherent meaning. Embeddings convert them into rich vectors where similar words live close together ‚Äî the foundation of how neural networks understand language.</p>
</div>
<hr class="div">

<!-- ‚ïê‚ïê‚ïê PART 1: THE PROBLEM ‚ïê‚ïê‚ïê -->
<div class="section" id="s1">
  <div class="stag">Part 01</div>
  <h2>Why Raw IDs Don't Work</h2>

  <p>After tokenization, we have token IDs: <code class="il">"The cat sat" ‚Üí [45, 892, 1203]</code>. These numbers are <strong>arbitrary labels</strong> ‚Äî there's no meaning in the numbers themselves. Token 45 ("the") is not "close to" token 46 in any meaningful sense, even though token 46 might be "a" (a very similar word).</p>

  <p>Here's why this matters: neural networks learn by computing distances, gradients, and similarities between numbers. If "cat" is ID 892 and "dog" is ID 7341, the network has no way to know these are related concepts. They're just two random numbers 6,449 apart. Meanwhile "cat" (892) and "cryptocurrency" (893) might be right next to each other numerically despite being completely unrelated.</p>

  <div class="analogy">
    <strong>üè† Analogy ‚Äî Street Addresses:</strong> Token IDs are like street addresses. 123 Main Street and 124 Main Street are numerically close but could house completely different businesses. Meanwhile, two bookstores at 123 Main and 8900 Oak are numerically far apart but semantically identical. We need a representation that puts "similar things" close together in some meaningful space ‚Äî not based on their arbitrary addresses, but on what they actually <em>are</em>.
  </div>

  <p>What we need is a way to convert each token ID into a <strong>rich vector</strong> of numbers ‚Äî a representation where proximity reflects meaning. That's what embeddings do.</p>
</div>
<hr class="div">

<!-- ‚ïê‚ïê‚ïê PART 2: LOOKUP TABLE ‚ïê‚ïê‚ïê -->
<div class="section" id="s2">
  <div class="stag">Part 02</div>
  <h2>Embeddings: A Learnable Lookup Table</h2>

  <p>An embedding is conceptually simple: it's a <strong>table</strong> where each row stores a vector for one token. Given a token ID, you look up the corresponding row. That's it ‚Äî one table lookup.</p>

  <pre><span class="cm"># Create an embedding table</span>
<span class="cm"># 1000 tokens, each represented by a 256-dimensional vector</span>
embedding = nn.<span class="fn">Embedding</span>(num_embeddings=<span class="num">1000</span>, embedding_dim=<span class="num">256</span>)

<span class="cm"># The table is a matrix of shape (1000, 256)</span>
<span class="cm"># Row 0 = vector for token 0</span>
<span class="cm"># Row 1 = vector for token 1</span>
<span class="cm"># ...</span>

<span class="cm"># Look up tokens [45, 892, 1203]</span>
ids = torch.<span class="fn">tensor</span>([<span class="num">45</span>, <span class="num">892</span>])
vectors = embedding(ids)   <span class="cm"># shape: (2, 256)</span>
<span class="cm"># Returns row 45 and row 892 from the table</span></pre>

  <p>The key insight: the values in this table are <strong>initially random</strong> but are <strong>learned during training</strong>. As the model trains, it adjusts the embedding vectors so that tokens used in similar contexts end up with similar vectors. This is how the model "discovers" that cat and dog are related.</p>

  <div class="cw">
    <div class="cw-title">‚ñ∏ Embedding Lookup Visualization</div>
    <p style="font-size:.88rem;color:var(--muted);margin-bottom:8px;">Click a token to highlight its row in the embedding table. Each row is one token's vector.</p>
    <div class="ctrls" id="lookCtrls">
      <button class="cbtn on" onclick="showLook(0)">ID 0: "the"</button>
      <button class="cbtn" onclick="showLook(1)">ID 1: "cat"</button>
      <button class="cbtn" onclick="showLook(2)">ID 2: "dog"</button>
      <button class="cbtn" onclick="showLook(3)">ID 3: "sat"</button>
      <button class="cbtn" onclick="showLook(4)">ID 4: "piano"</button>
    </div>
    <div style="display:flex;gap:20px;align-items:flex-start;flex-wrap:wrap;margin-top:10px;">
      <div>
        <div style="font-family:var(--mono);font-size:.75rem;color:var(--muted);margin-bottom:4px;">Embedding table (5 tokens √ó 8 dims)</div>
        <div id="embGrid"></div>
      </div>
      <div style="flex:1;min-width:200px;">
        <div style="font-family:var(--mono);font-size:.75rem;color:var(--muted);margin-bottom:4px;">Selected vector</div>
        <div class="out" id="embVec"></div>
        <div style="font-family:var(--mono);font-size:.75rem;color:var(--muted);margin-top:8px;">This vector IS the token's representation. The model uses these 8 numbers (768 in real GPT) to understand what "cat" means.</div>
      </div>
    </div>
  </div>

  <div class="callout">
    <strong>üìè Scale:</strong> GPT-2's embedding table has 50,257 tokens √ó 768 dimensions = <strong>38.6 million parameters</strong>. Each token is represented by 768 numbers. These 768 numbers encode everything the model knows about that token: its meaning, its grammatical role, what words typically appear near it, etc.
  </div>

  <!-- Quiz 1 -->
  <div class="qcard" id="q1">
    <div class="qq">üß© What is an embedding, mechanically?</div>
    <div class="qopts">
      <div class="qo" onclick="ck('q1',this,true)"><span class="ql">A</span> A lookup table that converts each token ID into a learned vector of numbers</div>
      <div class="qo" onclick="ck('q1',this,false)"><span class="ql">B</span> A compression algorithm that reduces text size</div>
      <div class="qo" onclick="ck('q1',this,false)"><span class="ql">C</span> A neural network layer that processes entire sentences at once</div>
      <div class="qo" onclick="ck('q1',this,false)"><span class="ql">D</span> A fixed mathematical formula applied to token IDs</div>
    </div>
    <div class="qfb" id="q1-fb"></div>
  </div>
</div>
<hr class="div">

<!-- ‚ïê‚ïê‚ïê PART 3: WHY THEY CAPTURE MEANING ‚ïê‚ïê‚ïê -->
<div class="section" id="s3">
  <div class="stag">Part 03</div>
  <h2>How Embeddings Capture Meaning</h2>

  <p>When the model trains on billions of sentences, it discovers that "cat" and "dog" appear in similar contexts: "The ___ sat on the mat", "I fed the ___", "My ___ is sleeping". Because they're <strong>used similarly</strong>, the training process pushes their embedding vectors to be <strong>close together</strong>.</p>

  <p>Meanwhile, "cat" and "refrigerator" appear in very different contexts, so their vectors drift apart.</p>

  <div class="analogy">
    <strong>üë• Analogy ‚Äî People at a Party:</strong> Imagine a party where people who share interests naturally drift together. Cat lovers cluster in one corner, sports fans in another, musicians somewhere else. Nobody is assigned a spot ‚Äî they just end up near people they have things in common with. Embeddings work the same way: during training, words that share contexts naturally end up near each other in the vector space.
  </div>

  <div class="cw">
    <div class="cw-title">‚ñ∏ Embedding Space (Simulated Trained Model)</div>
    <canvas id="simCanvas" height="300"></canvas>
    <p style="font-size:.88rem;color:var(--muted);margin-top:8px;">In a trained model, words cluster by meaning. "cat", "dog", "kitten" are close (all pets). "king", "queen", "prince" are close (all royalty). "computer", "laptop" are close (both technology). Each cluster is far from the others because these concepts appear in different contexts.</p>
  </div>

  <h3>The Famous king ‚àí man + woman ‚âà queen Result</h3>

  <p>In well-trained embeddings, <strong>vector arithmetic captures analogies</strong>. The direction from "man" to "woman" is the same as the direction from "king" to "queen". In vector math:</p>

  <pre><span class="cm"># The "gender" direction is captured in vector space</span>
king_vec - man_vec + woman_vec ‚âà queen_vec

<span class="cm"># More examples:</span>
paris_vec - france_vec + germany_vec ‚âà berlin_vec
bigger_vec - big_vec + small_vec ‚âà smaller_vec</pre>

  <p>This works because the embedding space has learned that "gender" is a consistent direction, "capital city" is another direction, "comparative form" is yet another. Each dimension captures some abstract feature of meaning.</p>

  <!-- Quiz 2 -->
  <div class="qcard" id="q2">
    <div class="qq">üß© Why do "cat" and "dog" end up with similar embeddings after training?</div>
    <div class="qopts">
      <div class="qo" onclick="ck('q2',this,false)"><span class="ql">A</span> They have similar spelling</div>
      <div class="qo" onclick="ck('q2',this,true)"><span class="ql">B</span> They appear in similar contexts in the training data ("The ___ sat on‚Ä¶")</div>
      <div class="qo" onclick="ck('q2',this,false)"><span class="ql">C</span> They have adjacent token IDs in the vocabulary</div>
      <div class="qo" onclick="ck('q2',this,false)"><span class="ql">D</span> A human manually set their embeddings to be similar</div>
    </div>
    <div class="qfb" id="q2-fb"></div>
  </div>
</div>
<hr class="div">

<!-- ‚ïê‚ïê‚ïê PART 4: POSITION PROBLEM ‚ïê‚ïê‚ïê -->
<div class="section" id="s4">
  <div class="stag">Part 04</div>
  <h2>The Position Problem</h2>

  <p>Here's a subtle but critical issue: <strong>"The dog bit the man"</strong> and <strong>"The man bit the dog"</strong> have very different meanings but contain the exact same words. If our embeddings only encode <em>what</em> each token is (its identity), the model can't tell these sentences apart.</p>

  <p>The root cause: the same token ID gets the same embedding vector regardless of where it appears. Token "the" at position 0 has the same vector as "the" at position 3.</p>

  <div class="cw">
    <div class="cw-title">‚ñ∏ The Same Token at Different Positions</div>
    <p style="font-size:.88rem;color:var(--muted);margin-bottom:8px;">Notice: "the" appears at positions 0 and 3. Without positional encoding, the model sees them as identical.</p>
    <div style="display:flex;gap:8px;flex-wrap:wrap;margin:10px 0;">
      <div style="text-align:center;padding:12px 16px;border-radius:8px;background:var(--vi-dim);border:1px solid rgba(136,96,240,.3);font-family:var(--mono);font-size:.88rem;"><div style="font-size:.7rem;color:var(--muted);">pos 0</div><span style="color:var(--violet);font-weight:600;">the</span></div>
      <div style="text-align:center;padding:12px 16px;border-radius:8px;background:var(--surface2);border:1px solid var(--border);font-family:var(--mono);font-size:.88rem;"><div style="font-size:.7rem;color:var(--muted);">pos 1</div>dog</div>
      <div style="text-align:center;padding:12px 16px;border-radius:8px;background:var(--surface2);border:1px solid var(--border);font-family:var(--mono);font-size:.88rem;"><div style="font-size:.7rem;color:var(--muted);">pos 2</div>bit</div>
      <div style="text-align:center;padding:12px 16px;border-radius:8px;background:var(--vi-dim);border:1px solid rgba(136,96,240,.3);font-family:var(--mono);font-size:.88rem;"><div style="font-size:.7rem;color:var(--muted);">pos 3</div><span style="color:var(--violet);font-weight:600;">the</span></div>
      <div style="text-align:center;padding:12px 16px;border-radius:8px;background:var(--surface2);border:1px solid var(--border);font-family:var(--mono);font-size:.88rem;"><div style="font-size:.7rem;color:var(--muted);">pos 4</div>man</div>
    </div>
    <div class="out">embedding("the" at pos 0) == embedding("the" at pos 3)?  YES ‚ùå
Without position info, the model can't distinguish word order!</div>
  </div>

  <div class="analogy">
    <strong>üìç Analogy ‚Äî Name Tags at a Meeting:</strong> Imagine everyone at a meeting wears a name tag. But the tags only show their name, not their role. You know who's in the room, but not who's the speaker, the audience, or the moderator. Position embeddings are like adding a role badge ‚Äî now you know not just <em>who</em> each person is, but <em>where</em> they are in the sequence.
  </div>

  <p>This isn't just a theoretical issue. Transformers (unlike RNNs) process all tokens simultaneously ‚Äî they have <strong>no built-in notion of order</strong>. Without explicit position information, "cat sat mat" and "mat sat cat" would be identical to the model.</p>
</div>
<hr class="div">

<!-- ‚ïê‚ïê‚ïê PART 5: POSITIONAL EMBEDDINGS ‚ïê‚ïê‚ïê -->
<div class="section" id="s5">
  <div class="stag">Part 05</div>
  <h2>Positional Embeddings</h2>

  <p>The solution: add a <strong>second embedding table</strong> that encodes position. Each position (0, 1, 2, 3‚Ä¶) gets its own learned vector, just like each token gets its own vector. Then we <strong>add them together</strong>:</p>

  <pre><span class="cm"># Two separate lookup tables</span>
token_emb = nn.<span class="fn">Embedding</span>(vocab_size, embed_dim)     <span class="cm"># WHAT the token is</span>
pos_emb   = nn.<span class="fn">Embedding</span>(max_length, embed_dim)    <span class="cm"># WHERE the token is</span>

<span class="cm"># For input_ids = [45, 892, 1203]:</span>
tok_vectors = token_emb(input_ids)            <span class="cm"># look up each token</span>
pos_vectors = pos_emb(torch.<span class="fn">arange</span>(<span class="num">3</span>))       <span class="cm"># look up positions [0, 1, 2]</span>

final = tok_vectors + pos_vectors              <span class="cm"># add them element-wise</span></pre>

  <p>After adding, the same word at different positions has <strong>different final vectors</strong>. The model can now distinguish "the" at position 0 from "the" at position 3.</p>

  <h3>Why addition? Why not concatenation?</h3>
  <p>You might wonder why we add the vectors rather than concatenating them. Addition keeps the dimensionality the same (768-d + 768-d = 768-d), while concatenation would double it (768-d ‚à• 768-d = 1536-d). Empirically, addition works just as well and is more memory-efficient. The model learns to "mix" token and position information within the same dimensions.</p>

  <div class="cw">
    <div class="cw-title">‚ñ∏ Token + Position = Final Embedding</div>
    <canvas id="combCanvas" height="210"></canvas>
    <p style="font-size:.88rem;color:var(--muted);margin-top:6px;">Each cell represents one dimension of the vector. The final embedding combines what the token is (purple) with where it appears (gold). The result (teal) encodes both simultaneously.</p>
  </div>

  <!-- Quiz 3 -->
  <div class="qcard" id="q3">
    <div class="qq">üß© Why do transformers need positional embeddings while RNNs do not?</div>
    <div class="qopts">
      <div class="qo" onclick="ck('q3',this,false)"><span class="ql">A</span> Transformers have smaller vocabularies</div>
      <div class="qo" onclick="ck('q3',this,true)"><span class="ql">B</span> Transformers process all tokens simultaneously with no built-in order, while RNNs process tokens one at a time in sequence</div>
      <div class="qo" onclick="ck('q3',this,false)"><span class="ql">C</span> RNNs don't care about word order either</div>
      <div class="qo" onclick="ck('q3',this,false)"><span class="ql">D</span> Positional embeddings are optional in transformers</div>
    </div>
    <div class="qfb" id="q3-fb"></div>
  </div>
</div>
<hr class="div">

<!-- ‚ïê‚ïê‚ïê PART 5b: WHAT DIMENSIONS ENCODE ‚ïê‚ïê‚ïê -->
<div class="section" id="s5b">
  <div class="stag">Part 05b</div>
  <h2>What Do Embedding Dimensions Represent?</h2>

  <p>Each of the 768 numbers in a GPT embedding vector captures some abstract feature. No single dimension has a clean, human-readable meaning like "animacy" or "verb tense" ‚Äî instead, features are <strong>distributed</strong> across many dimensions. But researchers have found that certain <strong>directions</strong> in the space encode interpretable concepts:</p>

  <div style="display:grid;grid-template-columns:auto 1fr;gap:8px 16px;margin:14px 0;font-size:.93rem;line-height:1.7;">
    <span style="color:var(--violet);font-family:var(--mono);font-size:.82rem;">Direction 1</span><span><strong>Gender:</strong> Moving along this direction transforms "king" ‚Üí "queen", "man" ‚Üí "woman", "actor" ‚Üí "actress". The model learned a consistent "gender axis."</span>
    <span style="color:var(--teal);font-family:var(--mono);font-size:.82rem;">Direction 2</span><span><strong>Tense:</strong> Moving along this direction transforms "walk" ‚Üí "walked", "run" ‚Üí "ran". The model captured a "past tense direction."</span>
    <span style="color:var(--gold);font-family:var(--mono);font-size:.82rem;">Direction 3</span><span><strong>Formality:</strong> Moving along this direction shifts between informal and formal language ‚Äî "gonna" vs. "going to", "wanna" vs. "want to."</span>
    <span style="color:var(--coral);font-family:var(--mono);font-size:.82rem;">Direction 4</span><span><strong>Sentiment:</strong> Words like "wonderful", "amazing", "great" cluster on one end; "terrible", "awful", "horrible" on the other.</span>
  </div>

  <p>No one explicitly programmed these directions ‚Äî the model <strong>discovered</strong> them from training data. The 768 dimensions create a rich, multi-dimensional space where language structure is encoded geometrically.</p>

  <div class="analogy">
    <strong>üó∫Ô∏è Analogy ‚Äî Map Coordinates:</strong> Think of a GPS coordinate. Latitude and longitude are just two numbers, but together they encode a precise location on Earth. Embedding dimensions work the same way ‚Äî 768 "coordinates" encode a precise location in "meaning space." Just as moving north on a map changes your latitude, moving along the "gender direction" in embedding space changes the concept from male to female.
  </div>

  <h3>Measuring Similarity: Cosine Similarity</h3>

  <p>To measure how similar two embeddings are, we use <strong>cosine similarity</strong> ‚Äî the cosine of the angle between two vectors. If two vectors point in the same direction, their cosine is 1 (identical). If they're perpendicular, it's 0 (unrelated). If they point in opposite directions, it's -1 (opposite meaning).</p>

  <pre><span class="kw">import</span> torch.nn.functional <span class="kw">as</span> F

<span class="cm"># Get embeddings for two words</span>
cat_vec = embedding(torch.<span class="fn">tensor</span>([cat_id]))   <span class="cm"># shape: (1, 768)</span>
dog_vec = embedding(torch.<span class="fn">tensor</span>([dog_id]))   <span class="cm"># shape: (1, 768)</span>

<span class="cm"># Cosine similarity</span>
sim = F.<span class="fn">cosine_similarity</span>(cat_vec, dog_vec)
<span class="cm"># Result: ~0.85  (very similar ‚Äî both are pets!)</span>

<span class="cm"># Compare cat vs. refrigerator</span>
sim2 = F.<span class="fn">cosine_similarity</span>(cat_vec, fridge_vec)
<span class="cm"># Result: ~0.12  (not very similar ‚Äî as expected)</span></pre>

  <div class="cw">
    <div class="cw-title">‚ñ∏ Similarity Examples (Trained Model)</div>
    <div style="display:grid;grid-template-columns:1fr auto;gap:6px 16px;margin:8px 0;font-size:.93rem;">
      <span>cat ‚Üî dog</span><span style="font-family:var(--mono);color:var(--teal);font-weight:600;">0.85</span>
      <span>cat ‚Üî kitten</span><span style="font-family:var(--mono);color:var(--teal);font-weight:600;">0.92</span>
      <span>king ‚Üî queen</span><span style="font-family:var(--mono);color:var(--teal);font-weight:600;">0.88</span>
      <span>king ‚Üî man</span><span style="font-family:var(--mono);color:var(--gold);font-weight:600;">0.65</span>
      <span>king ‚Üî banana</span><span style="font-family:var(--mono);color:var(--coral);font-weight:600;">0.15</span>
      <span>happy ‚Üî sad</span><span style="font-family:var(--mono);color:var(--gold);font-weight:600;">0.62</span>
      <span>happy ‚Üî joyful</span><span style="font-family:var(--mono);color:var(--teal);font-weight:600;">0.93</span>
    </div>
    <p style="font-size:.86rem;color:var(--muted);margin-top:6px;">Notice: "happy" and "sad" have moderate similarity (0.62) ‚Äî they're opposites but both describe emotions, so they share some contextual features. "happy" and "joyful" (0.93) are near-synonyms.</p>
  </div>
</div>
<hr class="div">

<!-- ‚ïê‚ïê‚ïê PART 5c: COMMON MISCONCEPTIONS ‚ïê‚ïê‚ïê -->
<div class="section" id="s5c">
  <div class="stag">Part 05c</div>
  <h2>Common Embedding Misconceptions</h2>

  <div style="display:grid;grid-template-columns:1fr 1fr;gap:8px;margin:14px 0;">
    <div style="background:var(--co-dim);border-radius:6px;padding:12px;font-size:.9rem;border-left:3px solid var(--coral)"><strong style="color:var(--coral)">‚ùå "Embedding values are fixed"</strong><br>Embeddings are frozen after pretraining, right?</div>
    <div style="background:var(--te-dim);border-radius:6px;padding:12px;font-size:.9rem;border-left:3px solid var(--teal)"><strong style="color:var(--teal)">‚úÖ Reality</strong><br>During fine-tuning, embeddings continue to update. They can shift to better represent domain-specific vocabulary. Some techniques freeze embeddings to save memory, but it's a choice.</div>
    <div style="background:var(--co-dim);border-radius:6px;padding:12px;font-size:.9rem;border-left:3px solid var(--coral)"><strong style="color:var(--coral)">‚ùå "Each dimension means one thing"</strong><br>Dimension 42 encodes "animacy", dimension 100 encodes "tense", etc.</div>
    <div style="background:var(--te-dim);border-radius:6px;padding:12px;font-size:.9rem;border-left:3px solid var(--teal)"><strong style="color:var(--teal)">‚úÖ Reality</strong><br>Features are <strong>distributed</strong> across many dimensions. No single dimension has a clean meaning. Concepts like "gender" or "tense" are encoded as directions through the space, involving many dimensions simultaneously.</div>
    <div style="background:var(--co-dim);border-radius:6px;padding:12px;font-size:.9rem;border-left:3px solid var(--coral)"><strong style="color:var(--coral)">‚ùå "More dimensions = always better"</strong><br>If 768 is good, 10,000 must be amazing.</div>
    <div style="background:var(--te-dim);border-radius:6px;padding:12px;font-size:.9rem;border-left:3px solid var(--teal)"><strong style="color:var(--teal)">‚úÖ Reality</strong><br>There's diminishing returns. More dimensions need more data to train well and more memory. GPT-2 uses 768, GPT-3 uses 12,288 ‚Äî but GPT-3 also has 1000√ó more training data. The right size depends on your data and compute budget.</div>
  </div>
</div>
<hr class="div">

<!-- ‚ïê‚ïê‚ïê PART 6: COMPLETE MODULE ‚ïê‚ïê‚ïê -->
<div class="section" id="s6">
  <div class="stag">Part 06</div>
  <h2>The Complete GPT Embedding Module</h2>

  <p>In GPT, the embedding module combines three things: token embeddings, position embeddings, and <strong>dropout</strong> (randomly zeroing some values during training to prevent overfitting).</p>

  <pre><span class="kw">class</span> <span class="fn">GPTEmbedding</span>(nn.Module):
    <span class="kw">def</span> <span class="fn">__init__</span>(self, vocab_size, embed_dim, max_length, dropout=<span class="num">0.1</span>):
        super().__init__()
        self.token_emb = nn.<span class="fn">Embedding</span>(vocab_size, embed_dim)
        self.pos_emb   = nn.<span class="fn">Embedding</span>(max_length, embed_dim)
        self.dropout   = nn.<span class="fn">Dropout</span>(dropout)

    <span class="kw">def</span> <span class="fn">forward</span>(self, input_ids):
        batch_size, seq_length = input_ids.shape

        <span class="cm"># Token embeddings: (batch, seq_len, embed_dim)</span>
        tok = self.token_emb(input_ids)

        <span class="cm"># Position indices: [0, 1, 2, ..., seq_len-1]</span>
        positions = torch.<span class="fn">arange</span>(seq_length, device=input_ids.device)
        positions = positions.<span class="fn">unsqueeze</span>(<span class="num">0</span>).<span class="fn">expand</span>(batch_size, -<span class="num">1</span>)

        <span class="cm"># Position embeddings: (batch, seq_len, embed_dim)</span>
        pos = self.pos_emb(positions)

        <span class="cm"># Combine and regularize</span>
        <span class="kw">return</span> self.dropout(tok + pos)</pre>

  <h3>Parameter Count</h3>
  <p>Let's calculate how many parameters this module has for GPT-2:</p>

  <div class="cw">
    <div class="cw-title">‚ñ∏ GPT-2 Embedding Parameters</div>
    <div style="font-family:var(--mono);font-size:.88rem;line-height:2;">
      <div>Token embedding:    <span style="color:var(--violet);">50,257 √ó 768</span> = <strong style="color:var(--violet);">38,597,376</strong></div>
      <div>Position embedding: <span style="color:var(--gold);">1,024 √ó 768</span>  = <strong style="color:var(--gold);">786,432</strong></div>
      <div style="border-top:1px solid var(--border);padding-top:4px;margin-top:4px;">Total:              <strong style="color:var(--teal);">39,383,808</strong> parameters</div>
    </div>
    <p style="font-size:.88rem;color:var(--muted);margin-top:8px;">That's ~39 million parameters just for the embedding layer! GPT-2 has 124 million parameters total, so the embedding layer is about <strong>32%</strong> of the entire model. This is why vocabulary size matters ‚Äî every token you add costs 768 extra parameters.</p>
  </div>

  <!-- Quiz 4 -->
  <div class="qcard" id="q4">
    <div class="qq">üß© In the GPT embedding module, what is the formula for the final output?</div>
    <div class="qopts">
      <div class="qo" onclick="ck('q4',this,false)"><span class="ql">A</span> token_embedding √ó position_embedding</div>
      <div class="qo" onclick="ck('q4',this,true)"><span class="ql">B</span> dropout(token_embedding + position_embedding)</div>
      <div class="qo" onclick="ck('q4',this,false)"><span class="ql">C</span> concatenate(token_embedding, position_embedding)</div>
      <div class="qo" onclick="ck('q4',this,false)"><span class="ql">D</span> token_embedding - position_embedding</div>
    </div>
    <div class="qfb" id="q4-fb"></div>
  </div>
</div>
<hr class="div">

<!-- ‚ïê‚ïê‚ïê PART 7: THE PIPELINE ‚ïê‚ïê‚ïê -->
<div class="section" id="s7">
  <div class="stag">Part 07</div>
  <h2>The Complete Pipeline So Far</h2>

  <p>Let's trace the full journey from text to the embedding that enters the transformer:</p>

  <div class="cw">
    <div class="cw-title">‚ñ∏ Text ‚Üí Embedding Pipeline</div>
    <div id="pipeline" style="font-family:var(--mono);font-size:.88rem;"></div>
  </div>

  <p>Each step transforms the data into a richer representation. Raw text becomes token strings, then numbers, then dense vectors. These vectors are what the transformer's attention mechanism actually operates on (that's Week 3!).</p>

  <!-- Quiz 5 -->
  <div class="qcard" id="q5">
    <div class="qq">üß© If you have a vocabulary of 10,000 tokens and an embedding dimension of 512, how many parameters does the token embedding table have?</div>
    <div class="qopts">
      <div class="qo" onclick="ck('q5',this,false)"><span class="ql">A</span> 10,000 + 512 = 10,512</div>
      <div class="qo" onclick="ck('q5',this,false)"><span class="ql">B</span> 10,000 √ó 512 √ó 2 = 10,240,000</div>
      <div class="qo" onclick="ck('q5',this,true)"><span class="ql">C</span> 10,000 √ó 512 = 5,120,000</div>
      <div class="qo" onclick="ck('q5',this,false)"><span class="ql">D</span> 512 √ó 512 = 262,144</div>
    </div>
    <div class="qfb" id="q5-fb"></div>
  </div>
</div>
<hr class="div">

<!-- ‚ïê‚ïê‚ïê SUMMARY ‚ïê‚ïê‚ïê -->
<div class="section" id="s8">
  <div class="stag">Summary</div>
  <h2>Key Takeaways</h2>
  <div class="sgrid">
    <div class="sitem"><code>Embedding = lookup table</code><div class="d">Each token ID maps to a learned vector of numbers. It's just a matrix row lookup.</div></div>
    <div class="sitem"><code>Learned during training</code><div class="d">Starts random. Training pushes similar words to have similar vectors.</div></div>
    <div class="sitem"><code>Position matters</code><div class="d">Transformers have no built-in order. Positional embeddings encode WHERE each token appears.</div></div>
    <div class="sitem"><code>final = tok + pos</code><div class="d">Token and position embeddings are added element-wise, then dropout is applied.</div></div>
    <div class="sitem"><code>Massive parameter cost</code><div class="d">GPT-2's embeddings alone = 39M parameters (32% of the model).</div></div>
    <div class="sitem"><code>Meaning from context</code><div class="d">"cat" and "dog" get similar vectors because they appear in similar sentences.</div></div>
  </div>
  <div class="callout"><strong>üöÄ Next up:</strong> Lesson 4 ‚Äî Data Pipeline. We'll put tokenization and embeddings together with Datasets and DataLoaders to build the complete pipeline that feeds training data to the model.</div>
</div>

</div>

<script>
window.addEventListener('scroll',()=>{const h=document.documentElement.scrollHeight-innerHeight;document.getElementById('pf').style.width=(h>0?scrollY/h*100:0)+'%'});
const obs=new IntersectionObserver(es=>es.forEach(e=>{if(e.isIntersecting)e.target.classList.add('vis')}),{threshold:.08});
document.querySelectorAll('.section').forEach(s=>obs.observe(s));

// ‚ïê‚ïê‚ïê Embedding lookup ‚ïê‚ïê‚ïê
const embTable=[];const words=['the','cat','dog','sat','piano'];
for(let i=0;i<5;i++){const row=[];for(let j=0;j<8;j++)row.push((Math.random()*2-1).toFixed(2));embTable.push(row)}
function showLook(idx){
  document.querySelectorAll('#lookCtrls .cbtn').forEach(b=>b.classList.remove('on'));event.target.classList.add('on');
  let html=`<div class="egrid" style="grid-template-columns:60px repeat(8,44px);">`;
  html+=`<div class="elabel"></div>`;for(let j=0;j<8;j++)html+=`<div class="elabel">d${j}</div>`;
  for(let i=0;i<5;i++){
    const hl=i===idx;
    html+=`<div class="elabel" style="text-align:right;padding-right:6px;${hl?'color:var(--violet);font-weight:600;':''}">${words[i]}</div>`;
    for(let j=0;j<8;j++){
      const v=parseFloat(embTable[i][j]);
      const bg=hl?`rgba(136,96,240,${Math.abs(v)/2+.15})`:`rgba(${v>0?'64,208,184':'240,104,104'},${Math.abs(v)/3})`;
      html+=`<div class="ecell" style="background:${bg};color:${hl?'#fff':'var(--muted)'};">${embTable[i][j]}</div>`;
    }
  }
  html+='</div>';
  document.getElementById('embGrid').innerHTML=html;
  document.getElementById('embVec').textContent=`embedding(${idx}) ‚Üí "${words[idx]}"
[${embTable[idx].join(', ')}]

This 8-dimensional vector is what the model
"sees" when it encounters "${words[idx]}".
(Real GPT uses 768 dimensions.)`;
}
showLook(0);

// ‚ïê‚ïê‚ïê Similarity scatter ‚ïê‚ïê‚ïê
function drawSim(){
  const c=document.getElementById('simCanvas'),dpr=devicePixelRatio||1,w=c.clientWidth;
  c.width=w*dpr;c.height=300*dpr;const ctx=c.getContext('2d');ctx.scale(dpr,dpr);
  const pts=[
    {x:.2,y:.5,l:'cat',c:'#8860f0'},{x:.26,y:.42,l:'dog',c:'#8860f0'},{x:.16,y:.58,l:'kitten',c:'#8860f0'},{x:.24,y:.56,l:'puppy',c:'#8860f0'},
    {x:.72,y:.22,l:'king',c:'#e8c040'},{x:.78,y:.3,l:'queen',c:'#e8c040'},{x:.7,y:.18,l:'prince',c:'#e8c040'},{x:.82,y:.24,l:'princess',c:'#e8c040'},
    {x:.5,y:.82,l:'computer',c:'#f06868'},{x:.56,y:.86,l:'laptop',c:'#f06868'},{x:.48,y:.78,l:'keyboard',c:'#f06868'},
  ];
  ctx.fillStyle='#161628';ctx.fillRect(0,0,w,300);
  ctx.strokeStyle='rgba(136,96,240,.15)';ctx.lineWidth=1;ctx.setLineDash([4,4]);
  ctx.beginPath();ctx.arc(.22*w,.5*280+10,65,0,Math.PI*2);ctx.stroke();
  ctx.strokeStyle='rgba(232,192,64,.15)';ctx.beginPath();ctx.arc(.75*w,.24*280+10,55,0,Math.PI*2);ctx.stroke();
  ctx.strokeStyle='rgba(240,104,104,.15)';ctx.beginPath();ctx.arc(.52*w,.82*280+10,45,0,Math.PI*2);ctx.stroke();
  ctx.setLineDash([]);
  pts.forEach(p=>{
    const px=p.x*w,py=p.y*280+10;
    ctx.beginPath();ctx.arc(px,py,5,0,Math.PI*2);ctx.fillStyle=p.c;ctx.fill();
    ctx.beginPath();ctx.arc(px,py,5,0,Math.PI*2);ctx.strokeStyle='rgba(255,255,255,.3)';ctx.lineWidth=1;ctx.stroke();
    ctx.font='500 11px "Manrope"';ctx.fillStyle=p.c;ctx.fillText(p.l,px+9,py+4);
  });
  // labels
  ctx.font='600 12px "DM Mono"';
  ctx.fillStyle='rgba(136,96,240,.5)';ctx.fillText('Animals',(.22*w)-20,(.5*280+10)+75);
  ctx.fillStyle='rgba(232,192,64,.5)';ctx.fillText('Royalty',(.75*w)-20,(.24*280+10)+65);
  ctx.fillStyle='rgba(240,104,104,.5)';ctx.fillText('Tech',(.52*w)-12,(.82*280+10)+55);
}
drawSim();window.addEventListener('resize',drawSim);

// ‚ïê‚ïê‚ïê Combination viz ‚ïê‚ïê‚ïê
function drawComb(){
  const c=document.getElementById('combCanvas'),dpr=devicePixelRatio||1,w=c.clientWidth;
  c.width=w*dpr;c.height=210*dpr;const ctx=c.getContext('2d');ctx.scale(dpr,dpr);ctx.clearRect(0,0,w,210);
  const cw=38,gap=3,cols=8,rowH=55;
  const labels=[{t:'Token emb',sub:'"cat"',c:'#8860f0'},{t:'Position emb',sub:'pos 1',c:'#e8c040'},{t:'Final output',sub:'tok + pos',c:'#40d0b8'}];
  const vals=[[.3,-.5,.8,.1,-.2,.6,-.4,.9],[.1,.2,-.3,.4,.1,-.1,.2,-.2],[.4,-.3,.5,.5,-.1,.5,-.2,.7]];
  const startX=Math.max(10,w/2-(cols*(cw+gap))/2-70);
  labels.forEach((lb,r)=>{
    const y=r*rowH+10;
    ctx.fillStyle=lb.c;ctx.font='500 11px "DM Mono"';ctx.fillText(lb.t,startX,y+18);
    ctx.fillStyle='var(--muted)';ctx.font='400 10px "DM Mono"';ctx.fillText(lb.sub,startX,y+32);
    for(let j=0;j<cols;j++){
      const x=startX+90+j*(cw+gap),v=vals[r][j];
      ctx.fillStyle=`rgba(${v>0?'64,208,184':'240,104,104'},${Math.abs(v)/1.5+.08})`;
      ctx.beginPath();ctx.roundRect(x,y+6,cw,cw,3);ctx.fill();
      ctx.fillStyle='#ddd';ctx.font='500 10px "DM Mono"';ctx.textAlign='center';
      ctx.fillText(v.toFixed(1),x+cw/2,y+6+cw/2+4);ctx.textAlign='left';
    }
    if(r<2){ctx.fillStyle='var(--muted)';ctx.font='600 18px "Manrope"';ctx.fillText(r===0?'+':'=',startX+90+cols*(cw+gap)+8,y+28)}
  });
}
drawComb();window.addEventListener('resize',drawComb);

// ‚ïê‚ïê‚ïê Pipeline ‚ïê‚ïê‚ïê
document.getElementById('pipeline').innerHTML=`
<div style="display:flex;flex-direction:column;gap:5px;color:var(--text);">
  <div style="padding:10px 14px;border:1px solid var(--border);border-radius:6px;"><span style="color:var(--muted);">1. Text:</span> <span style="color:var(--coral);">"The cat sat"</span></div>
  <div style="text-align:center;color:var(--muted);font-size:.85rem;">‚Üì tokenize (BPE)</div>
  <div style="padding:10px 14px;border:1px solid var(--border);border-radius:6px;"><span style="color:var(--muted);">2. Tokens:</span> <span style="color:var(--gold);">["The", "‚ñÅcat", "‚ñÅsat"]</span></div>
  <div style="text-align:center;color:var(--muted);font-size:.85rem;">‚Üì vocabulary lookup</div>
  <div style="padding:10px 14px;border:1px solid var(--border);border-radius:6px;"><span style="color:var(--muted);">3. IDs:</span> <span style="color:var(--violet);">[464, 3797, 3332]</span></div>
  <div style="text-align:center;color:var(--muted);font-size:.85rem;">‚Üì token_emb(ids) + pos_emb([0,1,2])</div>
  <div style="padding:10px 14px;border:1px solid var(--border);border-radius:6px;"><span style="color:var(--muted);">4. Vectors:</span> <span style="color:var(--teal);">shape (1, 3, 768) ‚Äî three 768-dim vectors</span></div>
  <div style="text-align:center;color:var(--muted);font-size:.85rem;">‚Üì dropout(0.1)</div>
  <div style="padding:10px 14px;border:1px solid var(--vi-dim);border-radius:6px;background:var(--vi-dim);"><span style="color:var(--violet);font-weight:600;">5. Ready for Transformer attention! ‚Üí</span></div>
</div>`;

// ‚ïê‚ïê‚ïê Quiz ‚ïê‚ïê‚ïê
function ck(id,el,ok){
  const card=document.getElementById(id);if(card.dataset.a)return;card.dataset.a='1';
  card.querySelectorAll('.qo').forEach(o=>o.classList.add('d'));
  const fb=document.getElementById(id+'-fb');
  if(ok){el.classList.add('c');fb.className='qfb p show';fb.textContent='‚úÖ Correct!';}
  else{el.classList.add('w');card.querySelectorAll('.qo').forEach(o=>{if(o.onclick&&o.onclick.toString().includes('true'))o.classList.add('c')});
    fb.className='qfb f show';
    const ex={
      q1:'An embedding is mechanically a lookup table (a matrix). You give it a token ID, it returns the corresponding row ‚Äî a vector of learned numbers. It\'s nn.Embedding in PyTorch.',
      q2:'Embeddings learn from context, not spelling. "cat" and "dog" appear in the same patterns ("The ___ sat on the mat", "I fed the ___"), so training pushes their vectors close together. "cat" and "car" share spelling but not context, so they end up far apart.',
      q3:'RNNs process tokens sequentially (one after another), so they inherently know token order. Transformers process all tokens simultaneously in parallel ‚Äî without positional embeddings, they literally can\'t tell "dog bit man" from "man bit dog".',
      q4:'GPT adds token and position embeddings element-wise, then applies dropout for regularization. Multiplication, concatenation, and subtraction are not used.',
      q5:'The embedding table has one row per token and one column per dimension. So 10,000 tokens √ó 512 dimensions = 5,120,000 parameters. Each parameter is one floating-point number that gets learned during training.'
    };
    fb.textContent='‚ùå '+ex[id];
  }
}
</script>
</body>
</html>

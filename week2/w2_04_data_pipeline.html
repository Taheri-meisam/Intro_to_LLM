<!DOCTYPE html>
<html lang="en">
<head>
<meta charset="UTF-8">
<meta name="viewport" content="width=device-width, initial-scale=1.0">
<title>Week 2 ¬∑ Lesson 4: Data Pipeline</title>
<link href="https://fonts.googleapis.com/css2?family=Bitter:wght@400;600;700&family=Source+Code+Pro:wght@400;500;600&family=Rubik:wght@400;500;600&display=swap" rel="stylesheet">
<style>
:root{
  --bg:#f2f0ec;--surface:#fff;--surface2:#eae6de;--border:#d4cec2;
  --text:#3a3632;--muted:#8a8278;--heading:#1a1612;
  --rust:#c85028;--ru-dim:rgba(200,80,40,.1);
  --navy:#2848a0;--na-dim:rgba(40,72,160,.1);
  --teal:#18887a;--te-dim:rgba(24,136,122,.1);
  --gold:#b89020;--go-dim:rgba(184,144,32,.1);
  --mono:'Source Code Pro',monospace;--serif:'Bitter',serif;--sans:'Rubik',sans-serif;
  --r:10px;
}
*{margin:0;padding:0;box-sizing:border-box}
body{font-family:var(--sans);background:var(--bg);color:var(--text);line-height:1.76;font-size:15.5px}
.container{max-width:760px;margin:0 auto;padding:48px 24px 100px}
.pbar{position:fixed;top:0;left:0;right:0;height:3px;background:var(--border);z-index:999}.pfill{height:100%;width:0%;background:var(--rust);transition:width .3s}
.hero{text-align:center;padding:32px 0 48px}.hero-tag{font-family:var(--mono);font-size:.76rem;color:var(--rust);letter-spacing:.14em;text-transform:uppercase}.hero h1{font-family:var(--serif);font-size:2.3rem;font-weight:700;color:var(--heading);margin:8px 0}.hero h1 em{color:var(--rust);font-style:normal}.hero p{color:var(--muted);font-size:1.05rem;max-width:560px;margin:0 auto}
.section{margin-bottom:56px;opacity:0;transform:translateY(20px);transition:all .5s}.section.vis{opacity:1;transform:none}.stag{font-family:var(--mono);font-size:.72rem;color:var(--rust);letter-spacing:.1em;text-transform:uppercase;margin-bottom:4px}.section h2{font-family:var(--serif);font-size:1.5rem;font-weight:700;color:var(--heading);margin-bottom:14px}.section h3{font-family:var(--serif);font-size:1.15rem;font-weight:600;color:var(--heading);margin:20px 0 10px}.section p{margin-bottom:12px}.section p strong{color:var(--heading)}
pre{background:var(--heading);color:#ddd8d0;border-radius:var(--r);padding:16px 18px;margin:14px 0;overflow-x:auto;font-family:var(--mono);font-size:.84rem;line-height:1.6}
code.il{font-family:var(--mono);font-size:.86em;color:var(--rust);background:var(--ru-dim);padding:2px 6px;border-radius:4px}
.kw{color:#e0a0c0}.fn{color:#80c8e0}.num{color:#e8c060}.str{color:#a0d880}.cm{color:#8a8278;font-style:italic}
.callout{background:var(--ru-dim);border:1px solid rgba(200,80,40,.2);border-radius:var(--r);padding:16px 18px;margin:18px 0;font-size:.93rem}.callout.info{background:var(--na-dim);border-color:rgba(40,72,160,.2)}.callout.green{background:var(--te-dim);border-color:rgba(24,136,122,.2)}
.analogy{border-left:3px solid var(--rust);padding:10px 16px;margin:16px 0;background:var(--ru-dim);border-radius:0 var(--r) var(--r) 0;font-size:.93rem}.analogy strong{color:var(--heading)}
hr.div{border:none;border-top:1px solid var(--border);margin:52px 0}
.cw{background:var(--surface);border:1px solid var(--border);border-radius:var(--r);padding:20px;margin:18px 0}.cw-title{font-family:var(--mono);font-size:.76rem;color:var(--rust);text-transform:uppercase;letter-spacing:.1em;margin-bottom:10px}
.ctrls{display:flex;flex-wrap:wrap;gap:8px;margin:10px 0}
.cbtn{font-family:var(--mono);font-size:.8rem;padding:7px 14px;border:1px solid var(--border);border-radius:6px;background:var(--surface2);color:var(--text);cursor:pointer;transition:all .2s}.cbtn:hover{border-color:var(--rust);color:var(--rust)}.cbtn.on{background:var(--ru-dim);border-color:var(--rust);color:var(--rust)}
.out{background:var(--heading);border-radius:6px;padding:14px 16px;font-family:var(--mono);font-size:.84rem;color:#a0d880;line-height:1.6;margin:10px 0;white-space:pre-wrap;min-height:36px}
.step{display:flex;gap:12px;margin:10px 0;align-items:flex-start}.step-n{font-family:var(--mono);font-size:.75rem;color:#fff;background:var(--rust);width:26px;height:26px;border-radius:50%;display:flex;align-items:center;justify-content:center;flex-shrink:0;margin-top:2px;font-weight:600}.step-c{flex:1}
.tgrid{display:inline-grid;gap:2px;margin:8px 0}
.tcell{width:54px;height:36px;display:flex;align-items:center;justify-content:center;font-family:var(--mono);font-size:.78rem;border-radius:3px;border:1px solid var(--border);background:var(--surface);transition:all .3s}.tcell.inp{border-color:var(--navy);background:var(--na-dim);color:var(--navy)}.tcell.tgt{border-color:var(--teal);background:var(--te-dim);color:var(--teal)}.tcell.pad{border-color:var(--border);background:var(--surface2);color:var(--muted);opacity:.5}.tcell.mask{border-color:var(--gold);background:var(--go-dim);color:var(--gold)}.tcell.hl{border-width:2px;font-weight:700;box-shadow:0 0 8px rgba(0,0,0,.1)}
.flow-step{display:flex;align-items:center;gap:10px;padding:12px 14px;border:1px solid var(--border);border-radius:8px;background:var(--surface);transition:all .3s;cursor:pointer;margin:4px 0}.flow-step:hover,.flow-step.on{border-color:var(--rust);background:var(--ru-dim)}.flow-num{font-family:var(--mono);font-size:.72rem;color:var(--rust);font-weight:600;width:22px}.flow-label{font-weight:500;font-size:.92rem}.flow-arrow{text-align:center;color:var(--muted);font-size:.85rem}
.qcard{background:var(--surface);border:1px solid var(--border);border-radius:var(--r);padding:22px;margin:18px 0}.qq{font-family:var(--serif);font-size:1.05rem;color:var(--heading);margin-bottom:12px}.qopts{display:flex;flex-direction:column;gap:7px}.qo{display:flex;align-items:center;gap:10px;padding:11px 14px;border:1px solid var(--border);border-radius:6px;cursor:pointer;font-size:.9rem;transition:all .2s}.qo:hover{border-color:var(--rust)}.qo.c{border-color:var(--teal);background:var(--te-dim);color:var(--teal)}.qo.w{border-color:var(--rust);background:var(--ru-dim);color:var(--rust)}.qo.d{pointer-events:none;opacity:.85}
.ql{width:24px;height:24px;border-radius:50%;display:flex;align-items:center;justify-content:center;font-family:var(--mono);font-size:.75rem;font-weight:600;background:var(--surface2);color:var(--muted);flex-shrink:0}.qo.c .ql{background:var(--teal);color:#fff}.qo.w .ql{background:var(--rust);color:#fff}
.qfb{margin-top:10px;padding:10px 12px;border-radius:6px;font-size:.88rem;display:none;line-height:1.6}.qfb.show{display:block}.qfb.p{background:var(--te-dim);color:var(--teal);border-left:3px solid var(--teal)}.qfb.f{background:var(--ru-dim);color:var(--rust);border-left:3px solid var(--rust)}
.sgrid{display:grid;grid-template-columns:1fr 1fr;gap:10px;margin:14px 0}.sitem{background:var(--surface);border:1px solid var(--border);border-radius:8px;padding:12px 14px}.sitem code{font-family:var(--mono);font-size:.84rem;color:var(--rust)}.sitem .d{font-size:.86rem;color:var(--muted);margin-top:3px}
@media(max-width:600px){.hero h1{font-size:1.8rem}.sgrid{grid-template-columns:1fr}.tcell{width:44px;height:30px;font-size:.72rem}}
</style>
</head>
<body>
<div class="pbar"><div class="pfill" id="pf"></div></div>
<div class="container">

<div class="hero">
  <div class="hero-tag">Week 2 ¬∑ Lesson 4</div>
  <h1><em>Data Pipeline</em></h1>
  <p>Time to put everything together. We'll build the complete pipeline that takes raw text and transforms it into training batches ‚Äî exactly what happens before training GPT.</p>
</div>
<hr class="div">

<!-- ‚ïê‚ïê‚ïê OVERVIEW ‚ïê‚ïê‚ïê -->
<div class="section" id="s1">
  <div class="stag">Overview</div>
  <h2>The Five Components</h2>

  <p>A language model data pipeline has five components, each transforming data one step closer to what the model needs. Click each component to understand its role:</p>

  <div class="cw">
    <div class="cw-title">‚ñ∏ Pipeline Components</div>
    <div id="flowArea">
      <div class="flow-step on" onclick="showFlow(0)"><span class="flow-num">01</span><span class="flow-label">Raw Text</span></div>
      <div class="flow-arrow">‚Üì</div>
      <div class="flow-step" onclick="showFlow(1)"><span class="flow-num">02</span><span class="flow-label">Tokenizer</span></div>
      <div class="flow-arrow">‚Üì</div>
      <div class="flow-step" onclick="showFlow(2)"><span class="flow-num">03</span><span class="flow-label">Dataset</span></div>
      <div class="flow-arrow">‚Üì</div>
      <div class="flow-step" onclick="showFlow(3)"><span class="flow-num">04</span><span class="flow-label">DataLoader</span></div>
      <div class="flow-arrow">‚Üì</div>
      <div class="flow-step" onclick="showFlow(4)"><span class="flow-num">05</span><span class="flow-label">Embedding</span></div>
    </div>
    <div class="out" id="flowOut"></div>
  </div>

  <div class="analogy">
    <strong>üè≠ Analogy ‚Äî A Factory Assembly Line:</strong> Think of the pipeline as a food processing factory. Raw ingredients (text) come in the door. The first station washes and chops them (tokenizer). The second station portions them into meals (dataset creates input/target pairs). The third station packages meals into boxes (DataLoader batches them). The fourth station labels each box with nutritional info (embedding converts IDs to vectors). Only then does the product reach the consumer (the model).
  </div>
</div>
<hr class="div">

<!-- ‚ïê‚ïê‚ïê PART 1: NEXT-TOKEN PREDICTION ‚ïê‚ïê‚ïê -->
<div class="section" id="s2">
  <div class="stag">Part 01</div>
  <h2>The Core Task: Next-Token Prediction</h2>

  <p>Before we build the pipeline, let's understand <strong>what we're training the model to do</strong>. Language models learn by predicting the next token. Given some context, what comes next?</p>

  <pre><span class="cm"># Given "The cat sat on the", predict "mat"</span>
<span class="cm"># Given "The cat sat on", predict "the"</span>
<span class="cm"># Given "The cat sat", predict "on"</span>
<span class="cm"># Given "The cat", predict "sat"</span>
<span class="cm"># Given "The", predict "cat"</span></pre>

  <p>The elegant trick: from a single sequence, we create <strong>multiple training examples</strong>. The <strong>input</strong> is all tokens except the last. The <strong>target</strong> is all tokens except the first (shifted right by one position). At every position, the model predicts the next token.</p>

  <div class="cw">
    <div class="cw-title">‚ñ∏ Input vs Target ‚Äî Click Positions</div>
    <p style="font-size:.88rem;color:var(--muted);margin-bottom:8px;">Click any position to see what the model is predicting at that point:</p>
    <div style="margin-bottom:8px;">
      <div style="font-family:var(--mono);font-size:.75rem;color:var(--navy);margin-bottom:4px;">INPUT: tokens[:-1] ‚Äî the context the model sees</div>
      <div class="tgrid" style="grid-template-columns:repeat(7,54px);" id="inputRow"></div>
      <div style="font-family:var(--mono);font-size:.75rem;color:var(--teal);margin:10px 0 4px;">TARGET: tokens[1:] ‚Äî what the model should predict</div>
      <div class="tgrid" style="grid-template-columns:repeat(7,54px);" id="targetRow"></div>
    </div>
    <div id="pairNote" style="font-family:var(--mono);font-size:.86rem;color:var(--muted);line-height:1.6;"></div>
  </div>

  <p>This design is beautifully efficient. A single sequence of 1024 tokens creates 1023 training examples simultaneously. The model learns to predict "cat" after "The", "sat" after "The cat", and so on ‚Äî all from the same data in one forward pass.</p>

  <!-- Quiz 1 -->
  <div class="qcard" id="q1">
    <div class="qq">üß© If your token sequence is [10, 20, 30, 40, 50], what are the input and target?</div>
    <div class="qopts">
      <div class="qo" onclick="ck('q1',this,false)"><span class="ql">A</span> Input: [10, 20, 30, 40, 50], Target: [10, 20, 30, 40, 50]</div>
      <div class="qo" onclick="ck('q1',this,true)"><span class="ql">B</span> Input: [10, 20, 30, 40], Target: [20, 30, 40, 50]</div>
      <div class="qo" onclick="ck('q1',this,false)"><span class="ql">C</span> Input: [10, 20], Target: [30, 40, 50]</div>
      <div class="qo" onclick="ck('q1',this,false)"><span class="ql">D</span> Input: [50, 40, 30, 20], Target: [10, 20, 30, 40]</div>
    </div>
    <div class="qfb" id="q1-fb"></div>
  </div>
</div>
<hr class="div">

<!-- ‚ïê‚ïê‚ïê PART 2: PADDING ‚ïê‚ïê‚ïê -->
<div class="section" id="s3">
  <div class="stag">Part 02</div>
  <h2>Padding & Attention Masks</h2>

  <p>In practice, we process multiple sequences at once in a <strong>batch</strong> (for GPU efficiency). But sequences have different lengths! To stack them into a tensor, we must make them all the same length by adding <strong>padding tokens</strong>.</p>

  <div class="cw">
    <div class="cw-title">‚ñ∏ Why Padding Is Needed</div>
    <div style="font-family:var(--mono);font-size:.84rem;margin-bottom:8px;">Three sequences of different lengths:</div>
    <div class="tgrid" style="grid-template-columns:repeat(6,54px);">
      <div class="tcell inp">the</div><div class="tcell inp">cat</div><div class="tcell inp">sat</div><div class="tcell pad">PAD</div><div class="tcell pad">PAD</div><div class="tcell pad">PAD</div>
      <div class="tcell inp">the</div><div class="tcell inp">dog</div><div class="tcell inp">ran</div><div class="tcell inp">in</div><div class="tcell inp">the</div><div class="tcell inp">park</div>
      <div class="tcell inp">I</div><div class="tcell inp">love</div><div class="tcell pad">PAD</div><div class="tcell pad">PAD</div><div class="tcell pad">PAD</div><div class="tcell pad">PAD</div>
    </div>
    <p style="font-size:.88rem;color:var(--muted);margin-top:8px;">The longest sequence has 6 tokens, so the others get padded to 6. The PAD token (usually ID 0) fills the extra slots.</p>
  </div>

  <p>But there's a problem: we don't want the model to learn from padding tokens! If the model sees "the cat sat PAD PAD PAD" and tries to predict what comes after "sat", the answer shouldn't be "PAD". So we create an <strong>attention mask</strong>: a matrix of 1s and 0s that tells the model which tokens are real (1) and which are padding (0).</p>

  <div class="cw">
    <div class="cw-title">‚ñ∏ Attention Masks</div>
    <div style="font-family:var(--mono);font-size:.75rem;color:var(--gold);margin-bottom:4px;">Attention Mask: 1 = real token, 0 = ignore (padding)</div>
    <div class="tgrid" style="grid-template-columns:repeat(6,54px);">
      <div class="tcell mask">1</div><div class="tcell mask">1</div><div class="tcell mask">1</div><div class="tcell pad">0</div><div class="tcell pad">0</div><div class="tcell pad">0</div>
      <div class="tcell mask">1</div><div class="tcell mask">1</div><div class="tcell mask">1</div><div class="tcell mask">1</div><div class="tcell mask">1</div><div class="tcell mask">1</div>
      <div class="tcell mask">1</div><div class="tcell mask">1</div><div class="tcell pad">0</div><div class="tcell pad">0</div><div class="tcell pad">0</div><div class="tcell pad">0</div>
    </div>
  </div>

  <div class="callout info">
    <strong>üí° Why -100 for padded targets?</strong> PyTorch's <code class="il">CrossEntropyLoss</code> has a special feature: when the target is -100, it <strong>ignores that position</strong> entirely ‚Äî zero loss, zero gradient. So we set padded target positions to -100. This way, the model only learns from real tokens and never wastes effort predicting padding.
  </div>

  <div class="analogy">
    <strong>üìù Analogy ‚Äî Multiple-Choice Test:</strong> Imagine a test where some students finish early and leave blanks. When grading, you only count the questions they answered ‚Äî you don't penalize them for blank questions. The attention mask is like a grading key that says "only grade these answers" for each student.
  </div>

  <!-- Quiz 2 -->
  <div class="qcard" id="q2">
    <div class="qq">üß© Why do we use -100 as the target value for padded positions?</div>
    <div class="qopts">
      <div class="qo" onclick="ck('q2',this,false)"><span class="ql">A</span> It's a special number that means "end of sequence"</div>
      <div class="qo" onclick="ck('q2',this,false)"><span class="ql">B</span> PyTorch requires negative numbers for padding</div>
      <div class="qo" onclick="ck('q2',this,true)"><span class="ql">C</span> CrossEntropyLoss ignores positions where target is -100, so the model doesn't learn from padding</div>
      <div class="qo" onclick="ck('q2',this,false)"><span class="ql">D</span> It reduces memory usage</div>
    </div>
    <div class="qfb" id="q2-fb"></div>
  </div>
</div>
<hr class="div">

<!-- ‚ïê‚ïê‚ïê PART 3: DATALOADER ‚ïê‚ïê‚ïê -->
<div class="section" id="s4">
  <div class="stag">Part 03</div>
  <h2>DataLoader: Batching & Shuffling</h2>

  <p>The DataLoader wraps around the Dataset and handles two critical tasks:</p>

  <div class="step"><div class="step-n">1</div><div class="step-c"><strong>Batching:</strong> Groups individual sequences into batches of, say, 32 sequences. GPUs are massively parallel ‚Äî processing 32 sequences takes barely longer than processing 1. This gives a ~32√ó speedup!</div></div>
  <div class="step"><div class="step-n">2</div><div class="step-c"><strong>Shuffling:</strong> Randomizes the order of examples each epoch. Without shuffling, the model sees examples in the same order every time, which can cause it to memorize patterns in the ordering rather than learning the actual language.</div></div>

  <pre><span class="cm"># The DataLoader handles all of this for you</span>
loader = DataLoader(
    dataset,
    batch_size=<span class="num">32</span>,          <span class="cm"># 32 sequences per batch</span>
    shuffle=<span class="kw">True</span>,            <span class="cm"># randomize order each epoch</span>
    collate_fn=collate_fn,   <span class="cm"># our padding function</span>
)

<span class="cm"># Each iteration gives you a batch:</span>
<span class="kw">for</span> batch <span class="kw">in</span> loader:
    input_ids = batch[<span class="str">'input_ids'</span>]       <span class="cm"># shape: (32, seq_len)</span>
    target_ids = batch[<span class="str">'target_ids'</span>]     <span class="cm"># shape: (32, seq_len)</span>
    attention_mask = batch[<span class="str">'attention_mask'</span>]  <span class="cm"># shape: (32, seq_len)</span></pre>

  <!-- Quiz 3 -->
  <div class="qcard" id="q3">
    <div class="qq">üß© Why is shuffling important in the DataLoader?</div>
    <div class="qopts">
      <div class="qo" onclick="ck('q3',this,false)"><span class="ql">A</span> To make the model train faster</div>
      <div class="qo" onclick="ck('q3',this,true)"><span class="ql">B</span> To prevent the model from memorizing the order of examples, improving generalization</div>
      <div class="qo" onclick="ck('q3',this,false)"><span class="ql">C</span> To reduce memory usage</div>
      <div class="qo" onclick="ck('q3',this,false)"><span class="ql">D</span> Shuffling is optional and has no effect on training</div>
    </div>
    <div class="qfb" id="q3-fb"></div>
  </div>
</div>
<hr class="div">

<!-- ‚ïê‚ïê‚ïê PART 3b: WHY NEXT-TOKEN PREDICTION ‚ïê‚ïê‚ïê -->
<div class="section" id="s4b">
  <div class="stag">Part 03b</div>
  <h2>Why Next-Token Prediction Works So Well</h2>

  <p>You might wonder: why does simply predicting the next token produce models that can write essays, answer questions, and write code? The answer is surprisingly deep.</p>

  <p>Consider what the model must learn to predict accurately in these contexts:</p>

  <div style="display:grid;grid-template-columns:1fr auto;gap:8px 16px;margin:14px 0;font-size:.93rem;line-height:1.7;">
    <span>"The capital of France is ___"</span><span style="color:var(--rust);font-weight:600;">‚Üí requires world knowledge</span>
    <span>"She picked up the ball and ___ it"</span><span style="color:var(--rust);font-weight:600;">‚Üí requires grammar</span>
    <span>"2 + 2 = ___"</span><span style="color:var(--rust);font-weight:600;">‚Üí requires arithmetic</span>
    <span>"def fibonacci(n):\n    if n <= 1:\n        return ___"</span><span style="color:var(--rust);font-weight:600;">‚Üí requires programming logic</span>
    <span>"He was sad because his dog ___"</span><span style="color:var(--rust);font-weight:600;">‚Üí requires emotional reasoning</span>
    <span>"In 1776, the United States ___"</span><span style="color:var(--rust);font-weight:600;">‚Üí requires historical knowledge</span>
  </div>

  <p>To predict the next token well in all these contexts, the model must implicitly learn <strong>grammar, facts, reasoning, common sense, and more</strong>. Next-token prediction is deceptively simple ‚Äî it's a single, clean objective that forces the model to learn everything about language.</p>

  <div class="analogy">
    <strong>üìñ Analogy ‚Äî Finishing Someone's Sentences:</strong> Imagine someone who can perfectly finish anyone's sentence in any context. To do this, they'd need to know grammar, vocabulary, world knowledge, the speaker's personality, cultural context, and logical reasoning. That's essentially what an LLM learns ‚Äî but from billions of sentences instead of personal experience.
  </div>

  <div class="callout info">
    <strong>üí° Key insight:</strong> Next-token prediction is also called <strong>"self-supervised learning"</strong> because the labels (targets) come from the data itself ‚Äî no human needs to manually label anything. The next word IS the label. This is why LLMs can train on trillions of words from the internet ‚Äî every sentence is automatically a training example.
  </div>
</div>
<hr class="div">

<!-- ‚ïê‚ïê‚ïê PART 3c: COLLATE FUNCTION ‚ïê‚ïê‚ïê -->
<div class="section" id="s4c">
  <div class="stag">Part 03c</div>
  <h2>The Collate Function: Where Padding Happens</h2>

  <p>The DataLoader needs a <strong>collate function</strong> that takes a list of individual examples (which may have different lengths) and combines them into a single padded batch. Here's what it does:</p>

  <pre><span class="kw">def</span> <span class="fn">collate_fn</span>(batch):
    <span class="cm"># batch = [{"input_ids": [...], "target_ids": [...]}, ...]</span>

    <span class="cm"># 1. Find the longest sequence in this batch</span>
    max_len = <span class="fn">max</span>(<span class="fn">len</span>(ex[<span class="str">'input_ids'</span>]) <span class="kw">for</span> ex <span class="kw">in</span> batch)

    <span class="cm"># 2. Pad each sequence to max_len</span>
    padded_inputs, padded_targets, masks = [], [], []
    <span class="kw">for</span> ex <span class="kw">in</span> batch:
        pad_len = max_len - <span class="fn">len</span>(ex[<span class="str">'input_ids'</span>])
        padded_inputs.<span class="fn">append</span>(ex[<span class="str">'input_ids'</span>] + [<span class="num">0</span>] * pad_len)      <span class="cm"># pad with 0</span>
        padded_targets.<span class="fn">append</span>(ex[<span class="str">'target_ids'</span>] + [-<span class="num">100</span>] * pad_len) <span class="cm"># pad with -100</span>
        masks.<span class="fn">append</span>([<span class="num">1</span>] * <span class="fn">len</span>(ex[<span class="str">'input_ids'</span>]) + [<span class="num">0</span>] * pad_len)      <span class="cm"># 1=real, 0=pad</span>

    <span class="cm"># 3. Stack into tensors</span>
    <span class="kw">return</span> {
        <span class="str">'input_ids'</span>:      torch.<span class="fn">tensor</span>(padded_inputs),
        <span class="str">'target_ids'</span>:     torch.<span class="fn">tensor</span>(padded_targets),
        <span class="str">'attention_mask'</span>: torch.<span class="fn">tensor</span>(masks),
    }</pre>

  <p>Notice the <strong>three different padding strategies</strong>: inputs get padded with 0 (the PAD token), targets get padded with -100 (so the loss function ignores them), and the mask gets padded with 0 (so attention ignores padding). Each serves a different purpose.</p>
</div>
<hr class="div">

<!-- ‚ïê‚ïê‚ïê PART 3d: COMMON BUGS ‚ïê‚ïê‚ïê -->
<div class="section" id="s4d">
  <div class="stag">Part 03d</div>
  <h2>Common Pipeline Bugs</h2>

  <p>When building your own data pipeline, watch out for these frequent mistakes:</p>

  <div style="margin:14px 0;">
    <div style="display:flex;gap:8px;margin:10px 0;align-items:flex-start;"><div style="font-family:var(--mono);font-size:.85rem;color:var(--rust);flex-shrink:0;min-width:24px;">üêõ</div><div><strong>Off-by-one in input/target split.</strong> If sequence = [a,b,c,d,e], input should be [a,b,c,d] and target [b,c,d,e]. A common bug: making both the same length as the original sequence, which creates a shape mismatch.</div></div>
    <div style="display:flex;gap:8px;margin:10px 0;align-items:flex-start;"><div style="font-family:var(--mono);font-size:.85rem;color:var(--rust);flex-shrink:0;min-width:24px;">üêõ</div><div><strong>Forgetting to set target padding to -100.</strong> If you pad targets with 0, the model tries to predict the PAD token after every real sequence ‚Äî learning nonsense. Always use -100 for target padding.</div></div>
    <div style="display:flex;gap:8px;margin:10px 0;align-items:flex-start;"><div style="font-family:var(--mono);font-size:.85rem;color:var(--rust);flex-shrink:0;min-width:24px;">üêõ</div><div><strong>Not reshaping logits for loss.</strong> The model outputs (batch, seq, vocab) but CrossEntropyLoss expects (N, vocab) for predictions and (N,) for targets. You need <code class="il">logits.view(-1, vocab_size)</code> and <code class="il">targets.view(-1)</code>.</div></div>
    <div style="display:flex;gap:8px;margin:10px 0;align-items:flex-start;"><div style="font-family:var(--mono);font-size:.85rem;color:var(--rust);flex-shrink:0;min-width:24px;">üêõ</div><div><strong>Data leakage in train/val split.</strong> If you split after creating sliding-window sequences, adjacent windows from the same text might end up in both train and val sets. Split the raw text first, then create windows separately for each split.</div></div>
  </div>

  <div class="callout green">
    <strong>‚úÖ Quick sanity check:</strong> After building your pipeline, verify these: (1) input and target shapes are equal, (2) target is shifted right by 1 from input, (3) padded target values are -100, (4) attention mask has 0s wherever there are PAD tokens, (5) <code class="il">decode(input[0])</code> looks like readable text.
  </div>
</div>
<hr class="div">

<!-- ‚ïê‚ïê‚ïê PART 4: FULL TRACE ‚ïê‚ïê‚ïê -->
<div class="section" id="s5">
  <div class="stag">Part 04</div>
  <h2>Complete Pipeline Trace</h2>

  <p>Let's trace a single piece of text through every component of the pipeline. Click to run the trace and watch data transform at each stage:</p>

  <div class="cw">
    <div class="cw-title">‚ñ∏ Step-by-Step Pipeline Trace</div>
    <button class="cbtn on" onclick="runTrace()">‚ñ∂ Run pipeline on "The cat sat on the mat."</button>
    <div class="out" id="traceOut" style="max-height:360px;overflow-y:auto;margin-top:10px;">Click "Run pipeline" to trace the data through each component.</div>
  </div>

  <p>This trace shows a single example, but in real training, the DataLoader would batch 32 of these together, pad them to equal length, create attention masks, and then feed the whole batch through the embedding module at once.</p>
</div>
<hr class="div">

<!-- ‚ïê‚ïê‚ïê PART 5: TRAINING PREVIEW ‚ïê‚ïê‚ïê -->
<div class="section" id="s6">
  <div class="stag">Part 05</div>
  <h2>Training Loop Preview</h2>

  <p>With the pipeline in place, the actual training loop is surprisingly short. Here's what it looks like, with commentary on each step:</p>

  <pre><span class="cm"># Setup</span>
model = GPTModel(vocab_size, embed_dim, ...)
optimizer = torch.optim.<span class="fn">Adam</span>(model.<span class="fn">parameters</span>(), lr=<span class="num">1e-4</span>)
criterion = nn.<span class="fn">CrossEntropyLoss</span>(ignore_index=-<span class="num">100</span>)

<span class="kw">for</span> epoch <span class="kw">in</span> <span class="fn">range</span>(num_epochs):
    <span class="kw">for</span> batch <span class="kw">in</span> dataloader:

        <span class="cm"># 1. Get data from pipeline</span>
        input_ids  = batch[<span class="str">'input_ids'</span>]   <span class="cm"># (batch, seq_len)</span>
        target_ids = batch[<span class="str">'target_ids'</span>]  <span class="cm"># (batch, seq_len)</span>

        <span class="cm"># 2. Forward pass: text ‚Üí embeddings ‚Üí transformer ‚Üí predictions</span>
        logits = model(input_ids)         <span class="cm"># (batch, seq_len, vocab_size)</span>

        <span class="cm"># 3. Compute loss: how wrong are our predictions?</span>
        loss = criterion(
            logits.<span class="fn">view</span>(-<span class="num">1</span>, vocab_size),  <span class="cm"># flatten to (batch*seq, vocab)</span>
            target_ids.<span class="fn">view</span>(-<span class="num">1</span>)           <span class="cm"># flatten to (batch*seq,)</span>
        )

        <span class="cm"># 4. Backward pass: compute gradients</span>
        optimizer.<span class="fn">zero_grad</span>()
        loss.<span class="fn">backward</span>()
        optimizer.<span class="fn">step</span>()</pre>

  <p>Notice <code class="il">ignore_index=-100</code> in the loss function ‚Äî this is exactly what makes padding work. The loss function automatically skips any position where the target is -100.</p>

  <p>Also notice <code class="il">logits.view(-1, vocab_size)</code> ‚Äî the model outputs a probability distribution over the <strong>entire vocabulary</strong> at every position. If there are 50,000 tokens in the vocabulary and a sequence of 64 tokens, the model produces 64 √ó 50,000 = 3.2 million numbers per sequence. The loss function compares each position's distribution against the actual next token.</p>

  <!-- Quiz 4 -->
  <div class="qcard" id="q4">
    <div class="qq">üß© What shape are the logits (model output) for a batch of 32 sequences, each 64 tokens long, with a vocabulary of 50,000?</div>
    <div class="qopts">
      <div class="qo" onclick="ck('q4',this,false)"><span class="ql">A</span> (32, 64)</div>
      <div class="qo" onclick="ck('q4',this,false)"><span class="ql">B</span> (32, 50000)</div>
      <div class="qo" onclick="ck('q4',this,true)"><span class="ql">C</span> (32, 64, 50000) ‚Äî a probability distribution over vocab at each position</div>
      <div class="qo" onclick="ck('q4',this,false)"><span class="ql">D</span> (64, 50000)</div>
    </div>
    <div class="qfb" id="q4-fb"></div>
  </div>
</div>
<hr class="div">

<!-- ‚ïê‚ïê‚ïê PART 6: COMPLETE ORDER ‚ïê‚ïê‚ïê -->
<div class="section" id="s7">
  <div class="stag">Part 06</div>
  <h2>The Complete Flow</h2>

  <pre><span class="cm"># THE COMPLETE DATA PIPELINE</span>

Raw Text
  ‚Üì  <span class="cm">tokenizer.encode()</span>
Token IDs
  ‚Üì  <span class="cm">Dataset: input=tokens[:-1], target=tokens[1:]</span>
Input/Target Pairs
  ‚Üì  <span class="cm">DataLoader: batch, shuffle, pad, mask</span>
Padded Batches + Attention Masks
  ‚Üì  <span class="cm">GPTEmbedding: token_emb + pos_emb + dropout</span>
Dense Vectors  ‚Üí  <span class="cm">Ready for Transformer!</span></pre>

  <p>This is <strong>exactly</strong> what happens before training GPT. In the Week 2 Project, you'll build this pipeline for Shakespeare's text. In Week 4, you'll connect a transformer model and train it to generate Shakespeare.</p>

  <!-- Quiz 5 -->
  <div class="qcard" id="q5">
    <div class="qq">üß© What is the correct order of the complete pipeline?</div>
    <div class="qopts">
      <div class="qo" onclick="ck('q5',this,false)"><span class="ql">A</span> Embed ‚Üí Tokenize ‚Üí Batch ‚Üí Train</div>
      <div class="qo" onclick="ck('q5',this,true)"><span class="ql">B</span> Tokenize ‚Üí Dataset (input/target) ‚Üí DataLoader (batch/pad) ‚Üí Embed ‚Üí Transformer</div>
      <div class="qo" onclick="ck('q5',this,false)"><span class="ql">C</span> Train ‚Üí Tokenize ‚Üí Embed ‚Üí Batch</div>
      <div class="qo" onclick="ck('q5',this,false)"><span class="ql">D</span> Batch ‚Üí Tokenize ‚Üí Dataset ‚Üí Embed</div>
    </div>
    <div class="qfb" id="q5-fb"></div>
  </div>
</div>
<hr class="div">

<!-- ‚ïê‚ïê‚ïê SUMMARY ‚ïê‚ïê‚ïê -->
<div class="section" id="s8">
  <div class="stag">Summary</div>
  <h2>Key Takeaways</h2>
  <div class="sgrid">
    <div class="sitem"><code>Next-token prediction</code><div class="d">Input = tokens[:-1], Target = tokens[1:]. Model predicts the next token at every position.</div></div>
    <div class="sitem"><code>Padding</code><div class="d">Sequences padded to equal length for batching. Attention masks tell model which tokens are real.</div></div>
    <div class="sitem"><code>-100 = ignore</code><div class="d">CrossEntropyLoss ignores positions where target is -100. This prevents learning from padding.</div></div>
    <div class="sitem"><code>DataLoader</code><div class="d">Batches (GPU efficiency) and shuffles (better generalization) the data automatically.</div></div>
    <div class="sitem"><code>Logits shape</code><div class="d">(batch, seq_len, vocab_size) ‚Äî a full probability distribution at each position.</div></div>
    <div class="sitem"><code>Exactly what GPT uses</code><div class="d">This pipeline is the real thing. Tokenize ‚Üí Dataset ‚Üí Batch ‚Üí Embed ‚Üí Transformer.</div></div>
  </div>
  <div class="callout green"><strong>üöÄ Next up:</strong> Week 2 Project ‚Äî Shakespeare Text Processor. You'll build this entire pipeline for Shakespeare's works, ready to train a GPT model in Week 4!</div>
</div>

</div>

<script>
window.addEventListener('scroll',()=>{const h=document.documentElement.scrollHeight-innerHeight;document.getElementById('pf').style.width=(h>0?scrollY/h*100:0)+'%'});
const obs=new IntersectionObserver(es=>es.forEach(e=>{if(e.isIntersecting)e.target.classList.add('vis')}),{threshold:.08});
document.querySelectorAll('.section').forEach(s=>obs.observe(s));

// ‚ïê‚ïê‚ïê Flow ‚ïê‚ïê‚ïê
const flowInfo=[
  'RAW TEXT\n"The cat sat on the mat. The dog ran in the park."\n\nJust a string. No structure, no numbers, nothing a model can use yet.',
  'TOKENIZER\nConverts text ‚Üí token IDs using encode():\n"the cat sat on the mat" ‚Üí [4, 5, 8, 2, 4, 12, 6]\n\nEach word maps to a number in the vocabulary.\nAlso provides decode() to convert IDs back to text.',
  'DATASET\nCreates input/target pairs for next-token prediction:\n  Input:  [4, 5, 8, 2, 4, 12]  ‚Üê all except last\n  Target: [5, 8, 2, 4, 12, 6]  ‚Üê all except first\n\nAt position 0: given token 4 ("the"), predict 5 ("cat")\nAt position 1: given tokens 4,5 ("the cat"), predict 8 ("sat")\n...and so on for every position.',
  'DATALOADER\nGroups sequences into batches and handles padding:\n  Batch size: 32 sequences per batch\n  Padding: shorter sequences padded with token 0\n  Attention mask: 1s for real tokens, 0s for padding\n  Shuffling: random order each epoch\n\nOutput shapes: (32, max_seq_len) for all three tensors.',
  'EMBEDDING MODULE\nConverts token IDs to dense vectors:\n  token_emb(ids)     ‚Üí (32, seq_len, 768)\n  pos_emb([0,1,2..]) ‚Üí (32, seq_len, 768)\n  final = dropout(tok + pos)\n\n‚úÖ Output: (32, seq_len, 768) ‚Äî ready for the transformer!'
];
function showFlow(i){
  document.querySelectorAll('.flow-step').forEach((s,j)=>s.classList.toggle('on',j===i));
  document.getElementById('flowOut').textContent=flowInfo[i];
}
showFlow(0);

// ‚ïê‚ïê‚ïê Input/Target pairs ‚ïê‚ïê‚ïê
const pairWords=['the','cat','sat','on','the','mat','dog','ran'];
const pairIds=[4,5,8,2,4,12,7,9];
function buildPairs(){
  const inp=document.getElementById('inputRow'),tgt=document.getElementById('targetRow');
  let ih='',th='';
  for(let i=0;i<7;i++){
    ih+=`<div class="tcell inp" onclick="hlPair(${i})" style="cursor:pointer" id="inp${i}">${pairWords[i]}</div>`;
    th+=`<div class="tcell tgt" onclick="hlPair(${i})" style="cursor:pointer" id="tgt${i}">${pairWords[i+1]}</div>`;
  }
  inp.innerHTML=ih;tgt.innerHTML=th;
}
function hlPair(i){
  for(let j=0;j<7;j++){
    const isHl=j===i;
    document.getElementById('inp'+j).classList.toggle('hl',isHl);
    document.getElementById('tgt'+j).classList.toggle('hl',isHl);
  }
  document.getElementById('pairNote').textContent=`Position ${i}: Given "${pairWords[i]}" (ID ${pairIds[i]}), predict "${pairWords[i+1]}" (ID ${pairIds[i+1]}).\n\nThe model sees tokens 0 through ${i} and must predict the token at position ${i+1}.`;
}
buildPairs();hlPair(0);

// ‚ïê‚ïê‚ïê Trace ‚ïê‚ïê‚ïê
function runTrace(){
  const out=document.getElementById('traceOut');
  const lines=[
    '‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê',
    '  COMPLETE PIPELINE TRACE',
    '‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê',
    '',
    '1. RAW TEXT',
    '   "The cat sat on the mat."',
    '',
    '2. TOKENIZER: encode()',
    '   Preprocess: "the cat sat on the mat ."',
    '   Lookup each token in vocabulary:',
    '     "the" ‚Üí 4,  "cat" ‚Üí 5,  "sat" ‚Üí 8',
    '     "on"  ‚Üí 2,  "the" ‚Üí 4,  "mat" ‚Üí 12,  "." ‚Üí 6',
    '   Result: [4, 5, 8, 2, 4, 12, 6]',
    '',
    '3. DATASET: input/target split',
    '   sequence = [4, 5, 8, 2, 4, 12, 6]',
    '   Input:  [4, 5, 8, 2, 4, 12]  ‚Üê tokens[:-1]',
    '   Target: [5, 8, 2, 4, 12, 6]  ‚Üê tokens[1:]',
    '',
    '   This creates 6 training examples:',
    '   ‚Ä¢ Given [the]           ‚Üí predict "cat"',
    '   ‚Ä¢ Given [the, cat]      ‚Üí predict "sat"',
    '   ‚Ä¢ Given [the, cat, sat] ‚Üí predict "on"',
    '   ‚Ä¢ ...etc.',
    '',
    '4. DATALOADER: batch + pad',
    '   Batch this with other sequences',
    '   Pad to max length in batch',
    '   Input shape:  (batch_size, 6)',
    '   Target shape: (batch_size, 6)',
    '   Mask shape:   (batch_size, 6)',
    '',
    '5. EMBEDDING: IDs ‚Üí vectors',
    '   token_emb([4, 5, 8, 2, 4, 12]) ‚Üí (1, 6, 128)',
    '   pos_emb([0, 1, 2, 3, 4, 5])    ‚Üí (1, 6, 128)',
    '   final = dropout(tok + pos)       ‚Üí (1, 6, 128)',
    '',
    '‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê',
    '  ‚úÖ READY FOR TRANSFORMER!',
    '  Input:  (batch, 6, 128) embedding tensor',
    '  Target: (batch, 6) token IDs to predict',
    '‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê',
  ];
  out.textContent='';let i=0;
  function add(){if(i<lines.length){out.textContent+=lines[i]+'\n';out.scrollTop=out.scrollHeight;i++;setTimeout(add,70)}}
  add();
}

// ‚ïê‚ïê‚ïê Quiz ‚ïê‚ïê‚ïê
function ck(id,el,ok){
  const card=document.getElementById(id);if(card.dataset.a)return;card.dataset.a='1';
  card.querySelectorAll('.qo').forEach(o=>o.classList.add('d'));
  const fb=document.getElementById(id+'-fb');
  if(ok){el.classList.add('c');fb.className='qfb p show';fb.textContent='‚úÖ Correct!';}
  else{el.classList.add('w');card.querySelectorAll('.qo').forEach(o=>{if(o.onclick&&o.onclick.toString().includes('true'))o.classList.add('c')});
    fb.className='qfb f show';
    const ex={
      q1:'Input is everything except the last token, target is everything except the first ‚Äî shifted right by one position. So [10,20,30,40] is the input and [20,30,40,50] is the target.',
      q2:'PyTorch\'s CrossEntropyLoss has a special feature: when target=-100, it produces zero loss and zero gradient for that position. This means the model completely ignores padded positions during training.',
      q3:'Without shuffling, the model sees examples in the same order every epoch. It can memorize patterns in the ordering (like "sports text always follows politics text") instead of learning actual language. Shuffling forces the model to generalize.',
      q4:'The model outputs a probability distribution over the entire vocabulary at every position: (batch=32, seq_len=64, vocab=50000). At each of the 64 positions, the model predicts which of the 50,000 tokens comes next.',
      q5:'The correct pipeline order is: Tokenize (text ‚Üí IDs) ‚Üí Dataset (create input/target pairs) ‚Üí DataLoader (batch + pad) ‚Üí Embed (IDs ‚Üí vectors) ‚Üí Transformer.'
    };
    fb.textContent='‚ùå '+ex[id];
  }
}
</script>
</body>
</html>

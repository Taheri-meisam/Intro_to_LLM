<!DOCTYPE html>
<html lang="en">
<head>
<meta charset="UTF-8">
<meta name="viewport" content="width=device-width, initial-scale=1.0">
<title>Week 2 Project: Shakespeare Text Processor</title>
<link href="https://fonts.googleapis.com/css2?family=Cormorant+Garamond:ital,wght@0,400;0,600;0,700;1,400&family=JetBrains+Mono:wght@400;500;600&family=Lato:wght@400;700&display=swap" rel="stylesheet">
<style>
:root{
  --bg:#1a0e10;--surface:#241418;--surface2:#2e1a1e;--border:#4a2830;
  --text:#d8c8c0;--muted:#907868;--heading:#f0e8e0;
  --wine:#c83858;--wi-dim:rgba(200,56,88,.12);--wi-glow:rgba(200,56,88,.25);
  --gold:#d8a830;--go-dim:rgba(216,168,48,.12);
  --sage:#58b878;--sa-dim:rgba(88,184,120,.12);
  --sky:#58a0d8;--sk-dim:rgba(88,160,216,.12);
  --mono:'JetBrains Mono',monospace;--serif:'Cormorant Garamond',serif;--sans:'Lato',sans-serif;
  --r:10px;
}
*{margin:0;padding:0;box-sizing:border-box}
body{font-family:var(--sans);background:var(--bg);color:var(--text);line-height:1.76;font-size:15.5px}
.container{max-width:760px;margin:0 auto;padding:48px 24px 100px;position:relative;z-index:1}
.pbar{position:fixed;top:0;left:0;right:0;height:3px;background:var(--surface);z-index:999}.pfill{height:100%;width:0%;background:var(--wine);box-shadow:0 0 8px var(--wi-glow);transition:width .3s}
.hero{text-align:center;padding:32px 0 48px}.hero-tag{font-family:var(--mono);font-size:.76rem;color:var(--wine);letter-spacing:.14em;text-transform:uppercase}.hero h1{font-family:var(--serif);font-size:2.8rem;font-weight:700;color:var(--heading);margin:8px 0;font-style:italic}.hero h1 em{color:var(--gold);font-style:normal}.hero p{color:var(--muted);font-size:1.05rem;max-width:560px;margin:0 auto}.hero .quote{font-family:var(--serif);font-style:italic;color:var(--muted);margin-top:12px;font-size:1.1rem}
.section{margin-bottom:56px;opacity:0;transform:translateY(20px);transition:all .5s}.section.vis{opacity:1;transform:none}.stag{font-family:var(--mono);font-size:.72rem;color:var(--wine);letter-spacing:.1em;text-transform:uppercase;margin-bottom:4px}.section h2{font-family:var(--serif);font-size:1.6rem;font-weight:700;color:var(--heading);margin-bottom:14px}.section h3{font-family:var(--serif);font-size:1.15rem;font-weight:600;color:var(--heading);margin:20px 0 10px}.section p{margin-bottom:12px}.section p strong{color:var(--heading)}
pre{background:var(--surface);border:1px solid var(--border);border-left:3px solid var(--wine);border-radius:var(--r);padding:16px 18px;margin:14px 0;overflow-x:auto;font-family:var(--mono);font-size:.84rem;line-height:1.6;color:var(--text)}
code.il{font-family:var(--mono);font-size:.86em;color:var(--wine);background:var(--wi-dim);padding:2px 6px;border-radius:4px}
.kw{color:#d8a0c0}.fn{color:#80c8e0}.num{color:#e8c060}.str{color:#a0d880}.cm{color:var(--muted);font-style:italic}
.callout{background:var(--wi-dim);border:1px solid rgba(200,56,88,.2);border-radius:var(--r);padding:16px 18px;margin:18px 0;font-size:.93rem}.callout.gold{background:var(--go-dim);border-color:rgba(216,168,48,.2)}.callout.green{background:var(--sa-dim);border-color:rgba(88,184,120,.2)}
.analogy{border-left:3px solid var(--gold);padding:10px 16px;margin:16px 0;background:var(--go-dim);border-radius:0 var(--r) var(--r) 0;font-size:.93rem}.analogy strong{color:var(--heading)}
hr.div{border:none;border-top:1px solid var(--border);margin:52px 0}
.cw{background:var(--surface);border:1px solid var(--border);border-radius:var(--r);padding:20px;margin:18px 0}.cw-title{font-family:var(--mono);font-size:.76rem;color:var(--wine);text-transform:uppercase;letter-spacing:.1em;margin-bottom:10px}
.cbtn{font-family:var(--mono);font-size:.8rem;padding:7px 14px;border:1px solid var(--border);border-radius:6px;background:var(--surface2);color:var(--text);cursor:pointer;transition:all .2s}.cbtn:hover{border-color:var(--wine);color:var(--wine)}.cbtn.on{background:var(--wi-dim);border-color:var(--wine);color:var(--wine)}
.out{background:#0e0608;border:1px solid var(--border);border-radius:6px;padding:14px 16px;font-family:var(--mono);font-size:.84rem;color:var(--sage);line-height:1.6;margin:10px 0;white-space:pre-wrap;min-height:36px}
.text-input{width:100%;padding:10px 14px;font-family:var(--sans);font-size:.95rem;border:1px solid var(--border);border-radius:6px;background:var(--surface2);color:var(--text);margin:8px 0;outline:none}.text-input:focus{border-color:var(--wine)}
.stats{display:grid;grid-template-columns:repeat(3,1fr);gap:10px;margin:16px 0}.stat{background:var(--surface2);border:1px solid var(--border);border-radius:8px;padding:14px;text-align:center}.stat-val{font-family:var(--serif);font-size:2rem;font-weight:700;color:var(--gold)}.stat-label{font-family:var(--mono);font-size:.72rem;color:var(--muted);text-transform:uppercase;letter-spacing:.06em;margin-top:2px}
.cgrid{display:flex;flex-wrap:wrap;gap:3px;margin:10px 0}.cchar{width:34px;height:34px;display:flex;align-items:center;justify-content:center;font-family:var(--mono);font-size:.8rem;border-radius:4px;border:1px solid var(--border);background:var(--surface2);color:var(--text);cursor:pointer;transition:all .2s}.cchar:hover,.cchar.hl{border-color:var(--gold);background:var(--go-dim);color:var(--gold)}
.qcard{background:var(--surface);border:1px solid var(--border);border-radius:var(--r);padding:22px;margin:18px 0}.qq{font-family:var(--serif);font-size:1.05rem;color:var(--heading);margin-bottom:12px}.qopts{display:flex;flex-direction:column;gap:7px}.qo{display:flex;align-items:center;gap:10px;padding:11px 14px;border:1px solid var(--border);border-radius:6px;cursor:pointer;font-size:.9rem;transition:all .2s}.qo:hover{border-color:var(--wine)}.qo.c{border-color:var(--sage);background:var(--sa-dim);color:var(--sage)}.qo.w{border-color:var(--wine);background:var(--wi-dim);color:var(--wine)}.qo.d{pointer-events:none;opacity:.85}
.ql{width:24px;height:24px;border-radius:50%;display:flex;align-items:center;justify-content:center;font-family:var(--mono);font-size:.75rem;font-weight:600;background:var(--surface2);color:var(--muted);flex-shrink:0}.qo.c .ql{background:var(--sage);color:#fff}.qo.w .ql{background:var(--wine);color:#fff}
.qfb{margin-top:10px;padding:10px 12px;border-radius:6px;font-size:.88rem;display:none;line-height:1.6}.qfb.show{display:block}.qfb.p{background:var(--sa-dim);color:var(--sage);border-left:3px solid var(--sage)}.qfb.f{background:var(--wi-dim);color:var(--wine);border-left:3px solid var(--wine)}
.sgrid{display:grid;grid-template-columns:1fr 1fr;gap:10px;margin:14px 0}
@media(max-width:600px){.hero h1{font-size:2rem}.stats{grid-template-columns:1fr 1fr}.sgrid{grid-template-columns:1fr}.cchar{width:28px;height:28px;font-size:.72rem}}
</style>
</head>
<body>
<div class="pbar"><div class="pfill" id="pf"></div></div>
<div class="container">

<div class="hero">
  <div class="hero-tag">Week 2 ¬∑ Project</div>
  <h1><em>Shakespeare</em> Text Processor</h1>
  <p>Build a complete text processing pipeline for Shakespeare's works ‚Äî the same pipeline you'll use in Week 4 to train a GPT model that generates Shakespeare-like text.</p>
  <div class="quote">"All the world's a stage, and all the men and women merely players."</div>
</div>
<hr class="div">

<!-- ‚ïê‚ïê‚ïê THE DATA ‚ïê‚ïê‚ïê -->
<div class="section" id="s1">
  <div class="stag">Step 1</div>
  <h2>The Data</h2>

  <p>We use excerpts from six of Shakespeare's most famous plays. In the real project (the Python script), you'd load a much larger Shakespeare dataset, but this subset demonstrates every concept.</p>

  <div class="cw">
    <div class="cw-title">‚ñ∏ Shakespeare Corpus</div>
    <div style="display:flex;flex-wrap:wrap;gap:6px;margin-bottom:10px;" id="passageBtns"></div>
    <div class="out" id="passageOut" style="max-height:160px;overflow-y:auto;color:var(--text);font-family:var(--serif);font-size:1.05rem;line-height:1.9;"></div>
  </div>

  <p>Notice how Shakespeare's text has unique challenges: old English words ("'tis", "wherefore", "hath"), heavy punctuation (commas, semicolons, colons), line breaks for verse, and apostrophes in unusual positions. Our tokenizer needs to handle all of this.</p>
</div>
<hr class="div">

<!-- ‚ïê‚ïê‚ïê TOKENIZER ‚ïê‚ïê‚ïê -->
<div class="section" id="s2">
  <div class="stag">Step 2</div>
  <h2>Character-Level Tokenizer</h2>

  <p>For this project, we use <strong>character-level tokenization</strong> rather than BPE. Why? Three reasons:</p>

  <div style="margin:14px 0;font-size:.93rem;line-height:1.8;">
    <strong>1. Simplicity.</strong> A character tokenizer requires no training ‚Äî just list all unique characters. BPE requires running the merge algorithm, which adds complexity we don't need for a small project.<br><br>
    <strong>2. Exact reproduction.</strong> Character-level tokenization perfectly preserves Shakespeare's formatting: line breaks, punctuation, capitalization, and old English spelling. Nothing gets merged or altered.<br><br>
    <strong>3. Small vocabulary.</strong> Shakespeare's text uses only ~65 unique characters. With BPE you'd have thousands of tokens. For a small model on a small corpus, fewer tokens = less to learn.
  </div>

  <p>The tradeoff: sequences are longer (each character is a token). But for a text generation project, this is actually fine ‚Äî the model learns character-by-character patterns and can generate perfectly formatted verse.</p>

  <div class="cw">
    <div class="cw-title">‚ñ∏ Character Vocabulary</div>
    <p style="font-size:.88rem;color:var(--muted);margin-bottom:8px;">Every unique character in the corpus becomes a token. Hover to see its ID:</p>
    <div class="cgrid" id="charGrid"></div>
    <div id="charCount" style="font-family:var(--mono);font-size:.84rem;color:var(--muted);margin-top:8px;"></div>
  </div>

  <div class="cw">
    <div class="cw-title">‚ñ∏ Try the Character Tokenizer</div>
    <input class="text-input" id="tokInput" value="To be, or not to be" oninput="runTok()">
    <div style="font-family:var(--mono);font-size:.75rem;color:var(--wine);margin:8px 0 4px;">Character tokens</div>
    <div class="cgrid" id="tokChars" style="gap:2px;"></div>
    <div style="font-family:var(--mono);font-size:.75rem;color:var(--sage);margin:8px 0 4px;">Token IDs</div>
    <div class="out" id="tokIds"></div>
    <div style="font-family:var(--mono);font-size:.75rem;color:var(--sky);margin:8px 0 4px;">Decoded back</div>
    <div class="out" id="tokDec" style="color:var(--sky);"></div>
  </div>

  <!-- Quiz 1 -->
  <div class="qcard" id="q1">
    <div class="qq">üß© Why does the Shakespeare project use character-level tokenization instead of BPE?</div>
    <div class="qopts">
      <div class="qo" onclick="ck('q1',this,false)"><span class="ql">A</span> BPE doesn't work with old English</div>
      <div class="qo" onclick="ck('q1',this,true)"><span class="ql">B</span> Character-level is simpler, preserves exact formatting, and works well for small corpora</div>
      <div class="qo" onclick="ck('q1',this,false)"><span class="ql">C</span> Character-level is always better than BPE</div>
      <div class="qo" onclick="ck('q1',this,false)"><span class="ql">D</span> Shakespeare's text is too short for BPE to work</div>
    </div>
    <div class="qfb" id="q1-fb"></div>
  </div>
</div>
<hr class="div">

<!-- ‚ïê‚ïê‚ïê DATASET ‚ïê‚ïê‚ïê -->
<div class="section" id="s3">
  <div class="stag">Step 3</div>
  <h2>Dataset: Sliding Window</h2>

  <p>We create training sequences by sliding a fixed-size window across the entire text. Each window position creates one input/target pair. The windows <strong>overlap</strong> ‚Äî they shift by one character each time ‚Äî which gives us the maximum number of training examples from the data.</p>

  <div class="analogy">
    <strong>üìú Analogy ‚Äî A Reading Magnifying Glass:</strong> Imagine sliding a magnifying glass across a scroll of Shakespeare's text. At each position, you see a fixed-width section. The text before the last character is the "context" (input), and the last character is what you're predicting (target). As you slide the glass one character at a time, each position creates a new training example.
  </div>

  <div class="cw">
    <div class="cw-title">‚ñ∏ Sliding Window Demo</div>
    <p style="font-size:.88rem;color:var(--muted);margin-bottom:6px;">Sequence length: 16 characters. Click ‚óÄ ‚ñ∂ to slide the window across the text.</p>
    <div style="display:flex;gap:8px;margin:8px 0;">
      <button class="cbtn" onclick="slideWin(-4)">‚óÄ Back 4</button>
      <button class="cbtn" onclick="slideWin(-1)">‚óÄ Back</button>
      <button class="cbtn on" onclick="slideWin(1)">Forward ‚ñ∂</button>
      <button class="cbtn" onclick="slideWin(4)">Forward 4 ‚ñ∂‚ñ∂</button>
    </div>
    <div id="winPos" style="font-family:var(--mono);font-size:.82rem;color:var(--muted);margin:6px 0;"></div>
    <div style="font-family:var(--mono);font-size:.75rem;color:var(--sky);margin:8px 0 4px;">INPUT (16 chars ‚Äî the context)</div>
    <div class="out" id="winInp" style="color:var(--sky);"></div>
    <div style="font-family:var(--mono);font-size:.75rem;color:var(--gold);margin:8px 0 4px;">TARGET (shifted by 1 ‚Äî what to predict)</div>
    <div class="out" id="winTgt" style="color:var(--gold);"></div>
    <div id="winEx" style="font-family:var(--mono);font-size:.82rem;color:var(--muted);margin-top:8px;"></div>
  </div>

  <pre><span class="kw">class</span> <span class="fn">ShakespeareDataset</span>(Dataset):
    <span class="kw">def</span> <span class="fn">__init__</span>(self, text, tokenizer, seq_length=<span class="num">64</span>):
        self.data = torch.<span class="fn">tensor</span>(tokenizer.<span class="fn">encode</span>(text))

    <span class="kw">def</span> <span class="fn">__getitem__</span>(self, idx):
        chunk = self.data[idx : idx + seq_length + <span class="num">1</span>]
        <span class="kw">return</span> {
            <span class="str">'input_ids'</span>:  chunk[:-<span class="num">1</span>],   <span class="cm"># first 64 chars</span>
            <span class="str">'target_ids'</span>: chunk[<span class="num">1</span>:]     <span class="cm"># shifted by 1</span>
        }

    <span class="kw">def</span> <span class="fn">__len__</span>(self):
        <span class="kw">return</span> <span class="fn">len</span>(self.data) - seq_length - <span class="num">1</span></pre>

  <!-- Quiz 2 -->
  <div class="qcard" id="q2">
    <div class="qq">üß© If the entire Shakespeare text is 2000 characters and seq_length is 64, how many training examples does the dataset create?</div>
    <div class="qopts">
      <div class="qo" onclick="ck('q2',this,false)"><span class="ql">A</span> 2000 / 64 = 31</div>
      <div class="qo" onclick="ck('q2',this,true)"><span class="ql">B</span> 2000 - 64 - 1 = 1935 (overlapping windows)</div>
      <div class="qo" onclick="ck('q2',this,false)"><span class="ql">C</span> 2000</div>
      <div class="qo" onclick="ck('q2',this,false)"><span class="ql">D</span> 64</div>
    </div>
    <div class="qfb" id="q2-fb"></div>
  </div>
</div>
<hr class="div">

<!-- ‚ïê‚ïê‚ïê STATS ‚ïê‚ïê‚ïê -->
<div class="section" id="s4">
  <div class="stag">Step 4</div>
  <h2>Pipeline Statistics</h2>

  <div class="stats" id="statsGrid"></div>

  <div class="cw">
    <div class="cw-title">‚ñ∏ Train / Validation Split</div>
    <p style="font-size:.88rem;color:var(--muted);margin-bottom:8px;">We reserve 10% of the data for validation ‚Äî text the model never trains on. This tells us if the model is actually learning language patterns (generalizing) or just memorizing the training data.</p>
    <div style="display:flex;height:28px;border-radius:6px;overflow:hidden;margin:8px 0;">
      <div style="flex:9;background:rgba(88,160,216,.3);display:flex;align-items:center;justify-content:center;font-family:var(--mono);font-size:.78rem;color:var(--sky);">Train 90%</div>
      <div style="flex:1;background:rgba(216,168,48,.3);display:flex;align-items:center;justify-content:center;font-family:var(--mono);font-size:.78rem;color:var(--gold);">Val 10%</div>
    </div>
  </div>

  <!-- Quiz 3 -->
  <div class="qcard" id="q3">
    <div class="qq">üß© Why do we split data into training and validation sets?</div>
    <div class="qopts">
      <div class="qo" onclick="ck('q3',this,false)"><span class="ql">A</span> To make training faster</div>
      <div class="qo" onclick="ck('q3',this,false)"><span class="ql">B</span> Because the model can only handle 90% of the data</div>
      <div class="qo" onclick="ck('q3',this,true)"><span class="ql">C</span> To measure if the model generalizes to unseen text, not just memorizing training data</div>
      <div class="qo" onclick="ck('q3',this,false)"><span class="ql">D</span> Validation data is used for the embedding table</div>
    </div>
    <div class="qfb" id="q3-fb"></div>
  </div>
</div>
<hr class="div">

<!-- ‚ïê‚ïê‚ïê FULL PIPELINE ‚ïê‚ïê‚ïê -->
<div class="section" id="s5">
  <div class="stag">Step 5</div>
  <h2>Complete Pipeline Test</h2>

  <div class="cw">
    <div class="cw-title">‚ñ∏ Run Complete Pipeline</div>
    <button class="cbtn on" onclick="runFull()">‚ñ∂ Trace full pipeline</button>
    <div class="out" id="fullOut" style="max-height:340px;overflow-y:auto;">Click to see data flow through every component.</div>
  </div>

  <!-- Quiz 4 -->
  <div class="qcard" id="q4">
    <div class="qq">üß© What will this pipeline be used for in Week 4?</div>
    <div class="qopts">
      <div class="qo" onclick="ck('q4',this,false)"><span class="ql">A</span> To translate Shakespeare into modern English</div>
      <div class="qo" onclick="ck('q4',this,true)"><span class="ql">B</span> To train a GPT model that generates new text in Shakespeare's style</div>
      <div class="qo" onclick="ck('q4',this,false)"><span class="ql">C</span> To classify Shakespeare's plays by genre</div>
      <div class="qo" onclick="ck('q4',this,false)"><span class="ql">D</span> To compress Shakespeare's text for storage</div>
    </div>
    <div class="qfb" id="q4-fb"></div>
  </div>
</div>
<hr class="div">

<!-- ‚ïê‚ïê‚ïê WHY SHAKESPEARE ‚ïê‚ïê‚ïê -->
<div class="section" id="s5b">
  <div class="stag">Deeper Dive</div>
  <h2>Why Shakespeare Is a Great First Dataset</h2>

  <p>Shakespeare's text has specific properties that make it ideal for learning how language models work:</p>

  <div style="margin:14px 0;font-size:.93rem;line-height:1.8;">
    <strong>Small but structured.</strong> The complete works are only ~5MB ‚Äî tiny by LLM standards. But the text has rich structure: iambic pentameter, rhyme schemes, recurring character names, stage directions. A small model can learn meaningful patterns quickly.<br><br>
    <strong>Distinctive style.</strong> You can immediately tell if the model's output looks Shakespearean. With generic web text, it's hard to know if the model learned anything useful. With Shakespeare, success is obvious: "Hark! What light through yonder window breaks" clearly came from a model trained on Shakespeare.<br><br>
    <strong>Repetitive patterns.</strong> Shakespeare reuses phrases, structures, and vocabulary. "To be or not to be", "Wherefore art thou", "What's in a name" ‚Äî these patterns give the model clear, repeated signals to learn from.<br><br>
    <strong>Character-level is natural.</strong> Shakespeare's text has unusual spelling, formatting, and punctuation that BPE might mangle. Character-level tokenization preserves everything perfectly, and the small vocabulary (~65 chars) means even a tiny model can learn it.
  </div>

  <div class="analogy">
    <strong>üéπ Analogy ‚Äî Learning Piano:</strong> You don't start piano by playing Rachmaninoff. You start with "Mary Had a Little Lamb" ‚Äî simple, structured, and you can immediately tell if you're playing it right. Shakespeare is the "Mary Had a Little Lamb" of language modeling: structured enough to be learnable, distinctive enough that success is obvious.
  </div>
</div>
<hr class="div">

<!-- ‚ïê‚ïê‚ïê HOW GENERATION WORKS ‚ïê‚ïê‚ïê -->
<div class="section" id="s5c">
  <div class="stag">Preview</div>
  <h2>How Text Generation Will Work</h2>

  <p>In Week 4, after training, you'll generate new Shakespeare. Here's a preview of how it works, step by step:</p>

  <div class="cw">
    <div class="cw-title">‚ñ∏ Generation Process (Character by Character)</div>
    <div class="out" style="color:var(--text);font-family:var(--serif);font-size:1.05rem;line-height:2;">
<span style="color:var(--sky);">Seed:</span>  "To be"
<span style="color:var(--gold);">Step 1:</span> Model sees "To be"     ‚Üí predicts "," (comma ‚Äî learned from "To be, or")
<span style="color:var(--gold);">Step 2:</span> Model sees "To be,"    ‚Üí predicts " " (space)
<span style="color:var(--gold);">Step 3:</span> Model sees "To be, "   ‚Üí predicts "o" (starting "or")
<span style="color:var(--gold);">Step 4:</span> Model sees "To be, o"  ‚Üí predicts "r"
<span style="color:var(--gold);">Step 5:</span> Model sees "To be, or" ‚Üí predicts " "
<span style="color:var(--gold);">Step 6:</span> Model sees "To be, or " ‚Üí predicts "n" (starting "not")
<span style="color:var(--gold);">...</span>
<span style="color:var(--sage);">Result:</span> "To be, or not to be, that is the dreaming..."
    </div>
    <p style="font-size:.88rem;color:var(--muted);margin-top:6px;">The model generates one character at a time, each time feeding back what it just predicted. It starts by reproducing memorized patterns ("To be, or not to be"), then gradually diverges into new text ("the dreaming..." instead of "the question"). This creative divergence is where the magic happens!</p>
  </div>

  <p>Three factors control how the output looks:</p>

  <div style="display:grid;grid-template-columns:auto 1fr;gap:8px 16px;margin:14px 0;font-size:.93rem;line-height:1.7;">
    <span style="font-family:var(--mono);font-size:.82rem;color:var(--wine);font-weight:600;">Temperature</span><span><strong>Controls randomness.</strong> Low temperature (0.1) = very predictable, sticks to the most likely next character. High temperature (1.5) = more creative and surprising but can become gibberish.</span>
    <span style="font-family:var(--mono);font-size:.82rem;color:var(--wine);font-weight:600;">Seed text</span><span><strong>The starting prompt.</strong> "To be" leads to Hamlet-style text. "Friends, Romans" leads to Julius Caesar-style. "Tomorrow" leads to Macbeth-style. The model continues in whatever style the seed suggests.</span>
    <span style="font-family:var(--mono);font-size:.82rem;color:var(--wine);font-weight:600;">Model size</span><span><strong>Capacity matters.</strong> A tiny model (10K params) might produce "To be orrr nnn ot to". A larger model (1M params) might produce "To be, or not to be, that is the dreaming of our mortality." More parameters = better language.</span>
  </div>

  <!-- Quiz 5 -->
  <div class="qcard" id="q5">
    <div class="qq">üß© During text generation, what does the model do at each step?</div>
    <div class="qopts">
      <div class="qo" onclick="ck('q5',this,false)"><span class="ql">A</span> Generates an entire sentence at once</div>
      <div class="qo" onclick="ck('q5',this,false)"><span class="ql">B</span> Looks up the next word in a database</div>
      <div class="qo" onclick="ck('q5',this,true)"><span class="ql">C</span> Predicts one character/token, appends it to the input, then repeats</div>
      <div class="qo" onclick="ck('q5',this,false)"><span class="ql">D</span> Randomly selects from the entire vocabulary</div>
    </div>
    <div class="qfb" id="q5-fb"></div>
  </div>
</div>
<hr class="div">

<!-- ‚ïê‚ïê‚ïê DONE ‚ïê‚ïê‚ïê -->
<div class="section" id="s6">
  <div class="stag">Complete</div>
  <h2>Week 2 ‚Äî Done! üéâ</h2>

  <p>You've built every component of a real text processing pipeline. Here's what each piece does and how they connect:</p>

  <div class="sgrid">
    <div style="background:var(--surface2);border:1px solid var(--border);border-radius:8px;padding:16px;"><div style="font-family:var(--mono);font-size:.82rem;color:var(--wine);margin-bottom:8px;">WHAT YOU BUILT</div><div style="font-size:.9rem;line-height:1.9;">‚úì Character tokenizer (encode + decode)<br>‚úì Dataset class with sliding window<br>‚úì DataLoader with batching + split<br>‚úì Embedding module (token + position)<br>‚úì Complete pipeline trace</div></div>
    <div style="background:var(--surface2);border:1px solid var(--border);border-radius:8px;padding:16px;"><div style="font-family:var(--mono);font-size:.82rem;color:var(--gold);margin-bottom:8px;">WHAT'S NEXT</div><div style="font-size:.9rem;line-height:1.9;">Week 3: Attention mechanisms<br>Week 4: Build + train a GPT model<br><br>The pipeline you built is exactly what feeds data into GPT during training.</div></div>
  </div>

  <div class="callout green">
    <strong>üé≠ The goal:</strong> In Week 4, you'll feed Shakespeare through this pipeline into a transformer, and the model will learn to write new text in Shakespeare's style. "To be, or not to be" might become "To dream, or not to dream" ‚Äî generated character by character.
  </div>
</div>

</div>

<script>
window.addEventListener('scroll',()=>{const h=document.documentElement.scrollHeight-innerHeight;document.getElementById('pf').style.width=(h>0?scrollY/h*100:0)+'%'});
const obs=new IntersectionObserver(es=>es.forEach(e=>{if(e.isIntersecting)e.target.classList.add('vis')}),{threshold:.08});
document.querySelectorAll('.section').forEach(s=>obs.observe(s));

// ‚ïê‚ïê‚ïê Shakespeare text ‚ïê‚ïê‚ïê
const passages={
  Hamlet:"To be, or not to be, that is the question:\nWhether 'tis nobler in the mind to suffer\nThe slings and arrows of outrageous fortune,\nOr to take arms against a sea of troubles\nAnd by opposing end them.",
  'As You Like It':"All the world's a stage,\nAnd all the men and women merely players;\nThey have their exits and their entrances,\nAnd one man in his time plays many parts.",
  'Julius Caesar':"Friends, Romans, countrymen, lend me your ears;\nI come to bury Caesar, not to praise him.\nThe evil that men do lives after them;\nThe good is oft interred with their bones.",
  'Romeo & Juliet':"O Romeo, Romeo, wherefore art thou Romeo?\nDeny thy father and refuse thy name.\nOr if thou wilt not, be but sworn my love\nAnd I'll no longer be a Capulet.",
  Macbeth:"Tomorrow, and tomorrow, and tomorrow,\nCreeps in this petty pace from day to day,\nTo the last syllable of recorded time;\nAnd all our yesterdays have lighted fools\nThe way to dusty death.",
  'The Tempest':"We are such stuff as dreams are made on,\nAnd our little life is rounded with a sleep.",
};
const fullText=Object.values(passages).join('\n\n');
const btnC=document.getElementById('passageBtns');
let first=true;
for(const k of Object.keys(passages)){
  const b=document.createElement('button');b.className='cbtn'+(first?' on':'');b.textContent=k;
  b.onclick=function(){btnC.querySelectorAll('.cbtn').forEach(x=>x.classList.remove('on'));this.classList.add('on');document.getElementById('passageOut').textContent=passages[k]};
  btnC.appendChild(b);first=false;
}
document.getElementById('passageOut').textContent=passages.Hamlet;

// ‚ïê‚ïê‚ïê Character vocabulary ‚ïê‚ïê‚ïê
const chars=[...new Set(fullText)].sort();
const c2id={'<PAD>':0};chars.forEach((c,i)=>c2id[c]=i+1);
const id2c={0:'<PAD>'};chars.forEach((c,i)=>id2c[i+1]=c);
const grid=document.getElementById('charGrid');
chars.forEach((c,i)=>{
  const el=document.createElement('div');el.className='cchar';
  el.textContent=c==='\n'?'‚Üµ':c===' '?'¬∑':c;
  el.title=`ID ${i+1}: ${JSON.stringify(c)}`;
  el.onmouseenter=()=>el.classList.add('hl');
  el.onmouseleave=()=>el.classList.remove('hl');
  grid.appendChild(el);
});
document.getElementById('charCount').textContent=`${chars.length} unique characters + 1 PAD token = ${chars.length+1} total vocabulary`;

// ‚ïê‚ïê‚ïê Tokenizer demo ‚ïê‚ïê‚ïê
function runTok(){
  const text=document.getElementById('tokInput').value;
  const cg=document.getElementById('tokChars');
  cg.innerHTML=[...text].map(c=>{
    const id=c2id[c]!==undefined?c2id[c]:'?';
    return `<div class="cchar hl" title="ID ${id}" style="width:28px;height:28px;font-size:.75rem;">${c==='\n'?'‚Üµ':c===' '?'¬∑':c}</div>`;
  }).join('');
  const ids=[...text].map(c=>c2id[c]!==undefined?c2id[c]:'?');
  document.getElementById('tokIds').textContent='['+ids.join(', ')+']  ('+ids.length+' tokens)';
  const decoded=[...text].map(c=>c2id[c]!==undefined?c:'?').join('');
  document.getElementById('tokDec').textContent=`"${decoded}"  ‚Üê ${decoded===text?'‚úÖ Perfect roundtrip!':'‚ö†Ô∏è Some chars unknown'}`;
}
runTok();

// ‚ïê‚ïê‚ïê Sliding window ‚ïê‚ïê‚ïê
let winIdx=0;const seqLen=16;
function slideWin(dir){
  winIdx=Math.max(0,Math.min(winIdx+dir,fullText.length-seqLen-2));
  const inp=fullText.slice(winIdx,winIdx+seqLen);
  const tgt=fullText.slice(winIdx+1,winIdx+seqLen+1);
  document.getElementById('winInp').textContent=JSON.stringify(inp);
  document.getElementById('winTgt').textContent=JSON.stringify(tgt);
  document.getElementById('winPos').textContent=`Window position: ${winIdx} of ${fullText.length-seqLen-1}`;
  const lastInput=inp[inp.length-1];const firstTarget=tgt[tgt.length-1];
  document.getElementById('winEx').textContent=`At this window: given context ending in "${lastInput==='\n'?'‚Üµ':lastInput}", predict "${firstTarget==='\n'?'‚Üµ':firstTarget}"`;
}
slideWin(0);

// ‚ïê‚ïê‚ïê Stats ‚ïê‚ïê‚ïê
const totalChars=fullText.length;const vocabSize=chars.length+1;const totalSeqs=totalChars-64-1;const trainSeqs=Math.floor(totalSeqs*.9);const valSeqs=totalSeqs-trainSeqs;
document.getElementById('statsGrid').innerHTML=`
  <div class="stat"><div class="stat-val">${totalChars.toLocaleString()}</div><div class="stat-label">Characters</div></div>
  <div class="stat"><div class="stat-val">${vocabSize}</div><div class="stat-label">Vocab Size</div></div>
  <div class="stat"><div class="stat-val">${totalSeqs>0?totalSeqs.toLocaleString():'‚Äî'}</div><div class="stat-label">Sequences (len 64)</div></div>
`;

// ‚ïê‚ïê‚ïê Full pipeline trace ‚ïê‚ïê‚ïê
function runFull(){
  const out=document.getElementById('fullOut');
  const sample=fullText.slice(0,Math.min(64,fullText.length));
  const ids=[...sample].map(c=>c2id[c]||0);
  const lines=[
    '‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê',
    '  SHAKESPEARE PIPELINE ‚Äî FULL TRACE',
    '‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê','',
    '1. RAW TEXT (first 64 characters):',
    `   "${sample.replace(/\n/g,'\\n')}"`,
    '','2. CHARACTER TOKENIZER:',
    `   ${sample.length} characters ‚Üí ${ids.length} token IDs`,
    `   IDs: [${ids.slice(0,15).join(', ')}, ...]`,
    `   Vocab size: ${vocabSize} (${chars.length} chars + PAD)`,
    '','3. DATASET (input/target split):',
    `   Input:  IDs[0:63]  ‚Üí 63 tokens`,
    `   Target: IDs[1:64]  ‚Üí 63 tokens`,
    '',
    '   Example predictions the model must learn:',
    `   Given "${sample[0]}", predict "${sample[1]}"  (ID ${ids[0]} ‚Üí ${ids[1]})`,
    `   Given "${sample.slice(0,2)}", predict "${sample[2]}"  (ID ${ids[1]} ‚Üí ${ids[2]})`,
    `   Given "${sample.slice(0,3)}", predict "${sample[3]}"`,
    '   ...63 predictions total from one window!',
    '','4. DATALOADER:',
    `   Batch size: 32`,
    `   Training sequences: ${trainSeqs>0?trainSeqs.toLocaleString():'N/A'}`,
    `   Validation sequences: ${valSeqs>0?valSeqs.toLocaleString():'N/A'}`,
    `   Batches per epoch: ${trainSeqs>0?Math.ceil(trainSeqs/32):'N/A'}`,
    '','5. EMBEDDING MODULE:',
    `   token_emb: (${vocabSize}, 64)   ‚Üí ${vocabSize*64} params`,
    `   pos_emb:   (128, 64)   ‚Üí ${128*64} params`,
    `   Total: ${(vocabSize*64+128*64).toLocaleString()} learnable parameters`,
    '','6. OUTPUT SHAPES:',
    `   Embedded: (32, 64, 64)`,
    `   (batch=32, seq_len=64, embed_dim=64)`,
    '',
    '‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê',
    '  ‚úÖ READY FOR TRANSFORMER!',
    '  In Week 4, this feeds into a GPT model',
    '  that learns to write like Shakespeare.',
    '‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê',
  ];
  out.textContent='';let i=0;
  function add(){if(i<lines.length){out.textContent+=lines[i]+'\n';out.scrollTop=out.scrollHeight;i++;setTimeout(add,60)}}
  add();
}

// ‚ïê‚ïê‚ïê Quiz ‚ïê‚ïê‚ïê
function ck(id,el,ok){
  const card=document.getElementById(id);if(card.dataset.a)return;card.dataset.a='1';
  card.querySelectorAll('.qo').forEach(o=>o.classList.add('d'));
  const fb=document.getElementById(id+'-fb');
  if(ok){el.classList.add('c');fb.className='qfb p show';fb.textContent='‚úÖ Correct!';}
  else{el.classList.add('w');card.querySelectorAll('.qo').forEach(o=>{if(o.onclick&&o.onclick.toString().includes('true'))o.classList.add('c')});
    fb.className='qfb f show';
    const ex={
      q1:'Character-level is simpler (no BPE training needed), preserves Shakespeare\'s exact formatting (line breaks, punctuation, old English), and works well for a small corpus with few unique characters.',
      q2:'With overlapping windows, each starting position creates one example. You can start a window at positions 0, 1, 2, ..., up to (2000-64-1). That\'s 1935 windows, each producing one training example. Overlap maximizes data usage!',
      q3:'Validation data is text the model never trains on. By measuring loss on validation data, we can tell if the model is actually learning language patterns (low val loss) or just memorizing training data (low train loss, high val loss = overfitting).',
      q4:'This pipeline feeds Shakespeare text into a GPT model that learns to predict the next character. After training, the model can generate new text in Shakespeare\'s style, producing original soliloquies and verses!',
      q5:'The model generates autoregressively: predict one token, append it, predict the next, append it, repeat. Each prediction considers all the text generated so far. This is why it\'s called "autoregressive" ‚Äî each output feeds back as input for the next step.'
    };
    fb.textContent='‚ùå '+ex[id];
  }
}
</script>
</body>
</html>
